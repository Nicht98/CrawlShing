{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"try: \n",
    "    from googlesearch import search \n",
    "except ImportError: \n",
    "    print(\"No module named 'google' found\") \n",
    "\n",
    "# to search \n",
    "query = \"phishing\"\n",
    "tab=[]\n",
    "for j in search(query, tld=\"com\", num=2, stop=2, pause=10): \n",
    "    tab.append(j)\n",
    "print(tab)\n",
    "with open('tes.txt', 'w') as f:\n",
    "    for item in tab:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "        \"\"\"\n",
    "from collections import deque\n",
    "from urllib.parse import urljoin\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import networkx as nx\n",
    "from networkx.algorithms import isomorphism\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from random import random\n",
    "from networkx.algorithms import isomorphism\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from networkx.algorithms import isomorphism\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from random import random\n",
    "import pandas as pd\n",
    "import trafilatura\n",
    "import sys \n",
    "import os        \n",
    "\n",
    "\n",
    "\n",
    "###preprocess\n",
    "\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def text_lowercase(text): \n",
    "    return text.lower() \n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    #words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def stem_and_lemmatize(words):   \n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return  lemmas\n",
    "\n",
    "####preprocess\n",
    "\n",
    "\n",
    "def content_extract(url,filename):\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        a1 = str(trafilatura.core.extract(downloaded)) \n",
    "        b= str(filename)\n",
    "        c= b+\".txt\"\n",
    "        a2=text_lowercase(a1)\n",
    "        a3=denoise_text(a2)\n",
    "        a4 = nltk.word_tokenize(a3)\n",
    "        a5 = normalize(a4)\n",
    "        a= stem_and_lemmatize(a5)   \n",
    "        text_file = open(c, \"w\")\n",
    "        text_file.write(str(a))   \n",
    "        text_file.close()\n",
    "        base=os.path.basename('Downloads/Compressed/test/'+c)\n",
    "        return    base        \n",
    "      \n",
    "        \n",
    "def graph_init():\n",
    "    i=0\n",
    "    j=1\n",
    "    txt = open(\"tes.txt\", \"r\")\n",
    "    s=txt.read().splitlines()\n",
    "    num_lines = sum(1 for line in open('tes.txt'))\n",
    "    for x in s:\n",
    "            content_extract(x,\"filename\"+str(i))\n",
    "            i=i+1\n",
    "graph_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Page to be crawled: https://discuss.hackbusters.com\n",
      "===================\n",
      "\n",
      "https://discuss.hackbusters.com/c/phishing/7\n",
      "https://discuss.hackbusters.com/c/phishing/7 6992.839509175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:trafilatura.core:not enough text None\n",
      "ERROR:trafilatura.core:not enough text None\n",
      "ERROR:trafilatura.core:not enough text None\n",
      "ERROR:trafilatura.core:not enough text None\n",
      "ERROR:trafilatura.core:not enough text None\n",
      "ERROR:trafilatura.core:not enough text None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://discuss.hackbusters.com/c/phishing/7\n",
      "https://discuss.hackbusters.com/c/phishing/7 7029.449552518\n",
      "https://discuss.hackbusters.com/tag/phishing\n",
      "https://discuss.hackbusters.com/tag/phishing 7045.992631941001\n",
      "https://discuss.hackbusters.com/tag/phishing\n",
      "https://discuss.hackbusters.com/tag/phishing 7061.442065701\n",
      "https://discuss.hackbusters.com/tag/phishing\n",
      "https://discuss.hackbusters.com/tag/phishing 7071.1313350460005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:trafilatura.core:not enough text None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://discuss.hackbusters.com/c/phishing/7?page=1\n",
      "https://discuss.hackbusters.com/c/phishing/7?page=1 7117.2365842809995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:trafilatura.core:not enough text None\n",
      "ERROR:trafilatura.core:not enough text None\n",
      "ERROR:trafilatura.core:not enough text None\n",
      "ERROR:trafilatura.core:not enough text None\n",
      "ERROR:trafilatura.core:not enough text None\n",
      "ERROR:trafilatura.core:not enough text None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://discuss.hackbusters.com/c/phishing/7\n",
      "https://discuss.hackbusters.com/c/phishing/7 7143.898896675\n",
      "https://discuss.hackbusters.com/tag/phishing\n",
      "https://discuss.hackbusters.com/tag/phishing 7159.95518861\n",
      "https://discuss.hackbusters.com/tag/phishing\n",
      "https://discuss.hackbusters.com/tag/phishing 7175.775554061\n",
      "https://discuss.hackbusters.com/tag/phishing\n",
      "https://discuss.hackbusters.com/tag/phishing 7185.909829647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:trafilatura.core:not enough text None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://discuss.hackbusters.com/c/phishing/7?page=1\n",
      "https://discuss.hackbusters.com/c/phishing/7?page=1 7236.120353717\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-5798c916358a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m \u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-5798c916358a>\u001b[0m in \u001b[0;36mcrawl\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;31m# Recursive call to crawl until the queue is populated with 100 URLs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m     \u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-5798c916358a>\u001b[0m in \u001b[0;36mcrawl\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;31m# Recursive call to crawl until the queue is populated with 100 URLs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m     \u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-5798c916358a>\u001b[0m in \u001b[0;36mcrawl\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvisited_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomplete_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msim_moyen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomplete_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m0.95\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mrobot_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomplete_url\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;33m:\u001b[0m  \u001b[1;31m# a modifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomplete_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-5798c916358a>\u001b[0m in \u001b[0;36msim_moyen\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msim_moyen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m     \u001b[0mcontent_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m     \u001b[1;31m#graph_init()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[0mtext0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sample.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-5798c916358a>\u001b[0m in \u001b[0;36mcontent_extractor\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcontent_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mdownloaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrafilatura\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrafilatura\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdownloaded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[0ma2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_lowercase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0ma3\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdenoise_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Software\\annaconda\\lib\\site-packages\\trafilatura\\core.py\u001b[0m in \u001b[0;36mextract\u001b[1;34m(filecontent, url, record_id, no_fallback, include_comments, output_format, csv_output, json_output, xml_output, tei_output, tei_validation, target_language, include_tables, include_formatting, date_extraction_params, with_metadata, url_blacklist)\u001b[0m\n\u001b[0;32m    665\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mno_fallback\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;31m#if sure_thing is False:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m         \u001b[0mpostbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompare_extraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackup_tree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpostbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_language\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;31m# rescue: try to use original/dirty tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Software\\annaconda\\lib\\site-packages\\trafilatura\\core.py\u001b[0m in \u001b[0;36mcompare_extraction\u001b[1;34m(tree, backup_tree, url, body, text, len_text, target_language)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[1;31m#    return body, text, len_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# try with readability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mtemppost_algo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtry_readability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackup_tree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[0malgo_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemppost_algo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitertext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[0mlen_algo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malgo_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Software\\annaconda\\lib\\site-packages\\trafilatura\\external.py\u001b[0m in \u001b[0;36mtry_readability\u001b[1;34m(htmlinput, url)\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevnull\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdevnull\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mredirect_stderr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevnull\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                     \u001b[0mresultstring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml_partial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0mnewtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhtml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresultstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mHTML_PARSER\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnewtree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Software\\annaconda\\lib\\site-packages\\readability\\readability.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(self, html_partial)\u001b[0m\n\u001b[0;32m    219\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_unlikely_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform_misused_divs_into_paragraphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m                 \u001b[0mcandidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_paragraphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[0mbest_candidate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_best_candidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Software\\annaconda\\lib\\site-packages\\readability\\readability.py\u001b[0m in \u001b[0;36mscore_paragraphs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mordered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[0mcandidate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m             \u001b[0mld\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_link_density\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"content_score\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m             log.debug(\n",
      "\u001b[1;32mF:\\Software\\annaconda\\lib\\site-packages\\readability\\readability.py\u001b[0m in \u001b[0;36mget_link_density\u001b[1;34m(self, elem)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;31m# if len(elem.findall(\".//div\") or elem.findall(\".//p\")):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[1;31m#    link_length = link_length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m         \u001b[0mtotal_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink_length\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Software\\annaconda\\lib\\site-packages\\readability\\readability.py\u001b[0m in \u001b[0;36mtext_length\u001b[1;34m(i)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtext_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Software\\annaconda\\lib\\site-packages\\lxml\\html\\__init__.py\u001b[0m in \u001b[0;36mtext_content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0many\u001b[0m \u001b[0mchildren\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m         \"\"\"\n\u001b[1;32m--> 419\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_collect_string_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcssselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'html'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msrc/lxml/xpath.pxi\u001b[0m in \u001b[0;36mlxml.etree.XPath.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/xpath.pxi\u001b[0m in \u001b[0;36mlxml.etree._XPathEvaluatorBase._handle_result\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/extensions.pxi\u001b[0m in \u001b[0;36mlxml.etree._unwrapXPathObject\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc/lxml/extensions.pxi\u001b[0m in \u001b[0;36mlxml.etree._elementStringResultFactory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import urllib.robotparser\n",
    "from urllib.parse import urljoin\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import networkx as nx\n",
    "from networkx.algorithms import isomorphism\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from random import random\n",
    "from networkx.algorithms import isomorphism\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "#import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from networkx.algorithms import isomorphism\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from random import random\n",
    "import pandas as pd\n",
    "import trafilatura\n",
    "import sys \n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###Module isomorphisme Debut\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_file(filename):    \n",
    "    with open(filename,'r') as f:\n",
    "        text = f.readlines()\n",
    "    return text\n",
    "\n",
    "def preprocess(line):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    line = [item.lower() for item in line if not item.lower() in stop_words]\n",
    "    return line\n",
    "\n",
    "def create_graph(text):\n",
    "    \n",
    "    word_list = []\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for line in text:\n",
    "        line = (line.strip()).split()\n",
    "        line = preprocess(line)\n",
    "        for i, word in enumerate(line):\n",
    "            if i != len(line)-1:\n",
    "                word_a = word\n",
    "                word_b = line[i+1]\n",
    "                if word_a not in word_list:\n",
    "                    word_list.append(word_a)\n",
    "                if word_b not in word_list:\n",
    "                    word_list.append(word_b)\n",
    "                if G.has_edge(word_a,word_b):\n",
    "                    G[word_a][word_b]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(word_a,word_b, weight = 1)\n",
    "  \n",
    "    return G        \n",
    "\n",
    "def search(word):\n",
    "    file = open(\"sample.txt\", \"r\")\n",
    "    data = file.read()\n",
    "    words=data.split()\n",
    "    found = re.findall('\\\\b' + word + '\\\\b', data)\n",
    "    if found:\n",
    "        #print(True, '{word} occurs {counts} time'.format(word=word, counts=found.count(word)))\n",
    "        counts=found.count(word)\n",
    "        tf=counts/len(words)\n",
    "    else:\n",
    "        counts=0\n",
    "        tf=0\n",
    "    return tf\n",
    "\n",
    "def similar(text,text1):  \n",
    "    G1=create_graph(text)\n",
    "    G2=create_graph(text1)\n",
    "    matching_graph=nx.Graph()\n",
    "\n",
    "    for n1,n2 in G2.edges():\n",
    "        if G1.has_edge(n1, n2):\n",
    "            matching_graph.add_edge(n1, n2)\n",
    "\n",
    "    components = nx.connected_components(matching_graph)\n",
    "\n",
    "    largest_component = max(components, key=len) \n",
    "    largest_common_subgraph = list( nx.induced_subgraph(matching_graph, largest_component))\n",
    "    \n",
    "    rslt = str(largest_common_subgraph)\n",
    "    x = rslt.split(\", \")\n",
    "\n",
    "    df = pd.DataFrame(x, columns = ['nodes']) \n",
    "    #print(df)\n",
    "    index = df.index\n",
    "    z = len(index)\n",
    "    if z==1 :  \n",
    "        a = len(index)-1\n",
    "    \n",
    "    else:\n",
    "        a = len(index)\n",
    "    #print(a)\n",
    "\n",
    "\n",
    "    dist = 1- a/ max(create_graph(text).number_of_nodes(),create_graph(text1).number_of_nodes())\n",
    "    \n",
    "  \n",
    "    return dist\n",
    "\n",
    "#erase pre exist files and save it alwyas to sample.txt\n",
    "def content_extractor(url):\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        a1 = str(trafilatura.core.extract(downloaded)) \n",
    "        a2=text_lowercase(a1)\n",
    "        a3=denoise_text(a2)\n",
    "        a4 = nltk.word_tokenize(a3)\n",
    "        a5 = normalize(a4)\n",
    "        a= stem_and_lemmatize(a5) \n",
    "        text_file = open(\"sample.txt\", \"w\")\n",
    "        n = text_file.write(str(a))\n",
    "        text_file.close()\n",
    "        \n",
    "\n",
    "        \n",
    "#create a file with the given name and return the name as output\n",
    "\"\"\"def content_extract(url,filename):\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        a = trafilatura.core.extract(downloaded) \n",
    "        b= str(filename)\n",
    "        c= b+\".txt\"\n",
    "        text_file = open(c, \"w\")\n",
    "        text_file.write(str(a))\n",
    "        text_file.close()\n",
    "        base=os.path.basename('Downloads/Compressed/test/'+c)\n",
    "        return    base\"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "#######debut preprocessing\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def text_lowercase(text): \n",
    "    return text.lower() \n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    #words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def stem_and_lemmatize(words):   \n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return  lemmas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def content_extract(url,filename):\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        a1 = str(trafilatura.core.extract(downloaded)) \n",
    "        b= str(filename)\n",
    "        c= b+\".txt\"\n",
    "        a2=text_lowercase(a1)\n",
    "        a3=denoise_text(a2)\n",
    "        a4 = nltk.word_tokenize(a3)\n",
    "        a5 = normalize(a4)\n",
    "        a= stem_and_lemmatize(a5)   \n",
    "        text_file = open(c, \"w\")\n",
    "        text_file.write(str(a))   \n",
    "        text_file.close()\n",
    "        base=os.path.basename('Downloads/Compressed/test/'+c)\n",
    "        return    base\n",
    "####### fin preprocessing      \n",
    "                              \n",
    "    \n",
    "#extract and create our set of text from google search url result     \n",
    "def graph_init():\n",
    "    i=0\n",
    "    txt = open(\"tes.txt\", \"r\")\n",
    "    s=txt.read().splitlines()\n",
    "    num_lines = sum(1 for line in open('tes.txt'))\n",
    "    for x in s:\n",
    "            content_extract(x,\"filename\"+str(i))\n",
    "            i=i+1\n",
    "            \n",
    "\n",
    "\n",
    "def test(filename): \n",
    "    chaine = \"['none']\"\n",
    "    a=str(filename)\n",
    "    fichier = open(a+\".txt\",\"r\")\n",
    "    for ligne in fichier:       \n",
    "        if chaine in ligne:           \n",
    "            fichier.close()\n",
    "            b=0\n",
    "        else:\n",
    "            b=1\n",
    "        return b\n",
    "    \n",
    "\n",
    "    \n",
    "def sim_moyen(url):\n",
    "    content_extractor(url)\n",
    "    #graph_init()\n",
    "    text0=load_file('sample.txt')    \n",
    "    text = load_file('filename0.txt')       \n",
    "    text1 = load_file('filename1.txt') \n",
    "    \n",
    "    try:\n",
    "        if test('sample')==0:\n",
    "            moyen=test('sample')+1\n",
    "        else:\n",
    "            sim1=similar(text0,text)-(search('phishing'))\n",
    "            sim2=similar(text0,text1)-(search('phishing'))\n",
    "            moyen=(sim1+sim2)/2 \n",
    "           # moyen=(similar(text0,text)+similar(text0,text1))/2 \n",
    "    except ValueError as ve:\n",
    "        moyen=1                      #when not common subgraph exist betwenn G1 annd G2\n",
    "    return moyen\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####Module isomorphisme Fin\n",
    "\n",
    "def robot_parser(url):\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url(url+'/robots.txt')\n",
    "    rp.read()\n",
    "    if not rp.can_fetch(\"*\", url):\n",
    "        a=0\n",
    "    else :\n",
    "        a=1\n",
    "    return a\n",
    "### robot parser\n",
    "\n",
    "\n",
    "\n",
    "# Read URL from command line\n",
    "url = \"https://discuss.hackbusters.com\"\n",
    "url1 = \"https://www.phishing.org/\"\n",
    "print (\"===================\")\n",
    "print (\"Page to be crawled:\", url)\n",
    "print( \"===================\")\n",
    "print()\n",
    "\n",
    "# Create queue\n",
    "queue = [(sim_moyen(url),url),]\n",
    "\n",
    "# Maintains list of visited pages\n",
    "visited_list = []\n",
    "# Crawl the page and populate the queue with newly found URLs\n",
    "\n",
    "\n",
    "def crawl(url):\n",
    "    start_time = timeit.default_timer()\n",
    "    visited_list.append((sim_moyen(url),url))\n",
    "    if len(queue) > 15:\n",
    "        return\n",
    "\n",
    "    urlf = urlopen(url)\n",
    "    soup = BeautifulSoup(urlf.read())\n",
    "    urls = soup.findAll(\"a\", href=True)\n",
    "\n",
    "    for i in urls:\n",
    "        flag = 0\n",
    "        # Complete relative URLs and strip trailing slash\n",
    "        complete_url = urljoin(url, i[\"href\"]).rstrip('/')\n",
    "\n",
    "        # Check if the URL already exists in the queue\n",
    "        for j in queue:\n",
    "            if j == complete_url:\n",
    "                flag = 1\n",
    "                break\n",
    "\n",
    "        # If not found in queue\n",
    "        if flag == 0 :\n",
    "            if len(queue) > 15:\n",
    "                \n",
    "                return\n",
    "            if ((visited_list.count(complete_url)) == 0 and sim_moyen(complete_url)<0.95 and robot_parser(complete_url) ==1)  :  # a modifier \n",
    "            \n",
    "                elapsed = timeit.default_timer() + start_time\n",
    "                print(complete_url,elapsed)\n",
    "                queue.append((sim_moyen(complete_url),complete_url))\n",
    "            \n",
    "    # Pop one URL from the queue from the left side so that it can be crawled\n",
    "    queue.sort(reverse=True)\n",
    "    current = queue.pop()[1]\n",
    "    \n",
    "    # Recursive call to crawl until the queue is populated with 100 URLs\n",
    "    crawl(current)\n",
    "\n",
    "crawl(url)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"# Print queue\n",
    "for i in queue:\n",
    "    print(sim_moyen(i)) \n",
    "    print (i)\n",
    "\n",
    "print()\n",
    "print (\"==============\")\n",
    "print (\"Pages crawled:\")\n",
    "print (\"==============\")\n",
    "print()\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Print list of visited pages\n",
    "for i in visited_list:\n",
    "    print(sim_moyen(i))\n",
    "    if sim_moyen(i) > 0.99: \n",
    "        \n",
    "        print (i)\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9304584865855949, 'https://discuss.hackbusters.com/c/phishing/7'), (0.9304584865855949, 'https://discuss.hackbusters.com/c/phishing/7'), (0.9328270275246249, 'https://discuss.hackbusters.com/c/phishing/7?page=1'), (0.9464219114219115, 'https://discuss.hackbusters.com/tag/phishing'), (0.9464219114219115, 'https://discuss.hackbusters.com/tag/phishing'), (0.9464219114219115, 'https://discuss.hackbusters.com/tag/phishing'), (0.9464219114219115, 'https://discuss.hackbusters.com/tag/phishing'), (0.9464219114219115, 'https://discuss.hackbusters.com/tag/phishing'), (0.9464219114219115, 'https://discuss.hackbusters.com/tag/phishing'), (0.9473064243072263, 'https://discuss.hackbusters.com/t/q1-2020-coronavirus-related-phishing-email-attacks-are-up-600/4915'), (0.9473064243072263, 'https://discuss.hackbusters.com/t/q1-2020-coronavirus-related-phishing-email-attacks-are-up-600/4915'), (0.9481250000000001, 'https://discuss.hackbusters.com/t/knowbe4-finds-coronavirus-themed-phishing-spiked-in-q2-2020-infographic/5084'), (0.948766108069244, 'https://discuss.hackbusters.com/t/coronavirus-phish-coming-fast-and-furious-do-you-intend-to-inoculate-your-users-get-the-latest-templates/4856'), (0.948766108069244, 'https://discuss.hackbusters.com/t/coronavirus-phish-coming-fast-and-furious-do-you-intend-to-inoculate-your-users-get-the-latest-templates/4856'), (0.9529834934712984, 'https://discuss.hackbusters.com/t/reporting-a-corporate-phishing-email-with-phisher-activated-it/5152'), (1, 'https://discuss.hackbusters.com/t/heads-up-cybercriminals-launch-phishing-campaign-to-capitalize-on-presidents-health/5174')]\n"
     ]
    }
   ],
   "source": [
    "queue.sort\n",
    "print(queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "def search(word):\n",
    "    file = open(\"sample.txt\", \"r\")\n",
    "    data = file.read()\n",
    "    words=data.split()\n",
    "    found = re.findall('\\\\b' + word + '\\\\b', data)\n",
    "    if found:\n",
    "        #print(True, '{word} occurs {counts} time'.format(word=word, counts=found.count(word)))\n",
    "        counts=found.count(word)\n",
    "        tf=counts/len(words)\n",
    "    else:\n",
    "        counts=0\n",
    "        tf=0\n",
    "    return tf\n",
    "print(type (search('none')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def search():\n",
    "    word='phishing'\n",
    "    k = 0\n",
    "    with open(\"sample.txt\", 'r') as f:\n",
    "        for line in f:\n",
    "            words = line.split()\n",
    "            for i in words:\n",
    "                if(i==word):\n",
    "                    k=k+1\n",
    "\n",
    "\n",
    "    return k/len(words)\n",
    "search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0.0009017180000228109 url hello\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "a='hello'\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"time:\",elapsed,\"url\",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
