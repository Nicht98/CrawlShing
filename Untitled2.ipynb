   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import isomorphism\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from random import random\n",
    "import pandas as pd\n",
    "def load_file(filename):\n",
    "    \n",
    "    with open(filename,'r') as f:\n",
    "        text = f.readlines()\n",
    "    return text\n",
    "\n",
    "def preprocess(line):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    line = [item.lower() for item in line if not item.lower() in stop_words]\n",
    "    return line\n",
    "\n",
    "def create_graph(text):\n",
    "    \n",
    "    word_list = []\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for line in text:\n",
    "        line = (line.strip()).split()\n",
    "        line = preprocess(line)\n",
    "        for i, word in enumerate(line):\n",
    "            if i != len(line)-1:\n",
    "                word_a = word\n",
    "                word_b = line[i+1]\n",
    "                if word_a not in word_list:\n",
    "                    word_list.append(word_a)\n",
    "                if word_b not in word_list:\n",
    "                    word_list.append(word_b)\n",
    "                if G.has_edge(word_a,word_b):\n",
    "                    G[word_a][word_b]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(word_a,word_b, weight = 1)\n",
    "    G1=G\n",
    "    #G1=nx.convert_node_labels_to_integers(G)    \n",
    "    return G1\n",
    "    \n",
    "\n",
    "\"\"\"def draw_final_graph(text_network):\n",
    "    \n",
    "    pos = nx.spring_layout(text_network,scale=2,k=0.5)  \n",
    "    nx.draw(text_network,pos,with_labels=False,font_size=8,width=1,node_size=500)\n",
    "    plt.draw()    \n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#graph2\n",
    "\n",
    "def load_file(filename):\n",
    "    \n",
    "    with open(filename,'r') as f:\n",
    "        text1 = f.readlines()\n",
    "    return text1\n",
    "\n",
    "def preprocess(line):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    line = [item.lower() for item in line if not item.lower() in stop_words]\n",
    "    return line\n",
    "\n",
    "def create_graph(text1):\n",
    "    \n",
    "    word_list = []\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for line in text1:\n",
    "        line = (line.strip()).split()\n",
    "        line = preprocess(line)\n",
    "        for i, word in enumerate(line):\n",
    "            if i != len(line)-1:\n",
    "                word_a = word\n",
    "                word_b = line[i+1]\n",
    "                if word_a not in word_list:\n",
    "                    word_list.append(word_a)\n",
    "                if word_b not in word_list:\n",
    "                    word_list.append(word_b)\n",
    "                if G.has_edge(word_a,word_b):\n",
    "                    G[word_a][word_b]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(word_a,word_b, weight = 1)\n",
    "    G2=G\n",
    "    #G2=nx.convert_node_labels_to_integers(G)\n",
    "    return G2 \n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    text = load_file('filename0.txt')\n",
    "    text_network = create_graph(text)\n",
    "    \n",
    "    text1 = load_file('filename.txt')\n",
    "    text_network = create_graph(text1)\n",
    "    create_graph(text)\n",
    "    create_graph(text1)\n",
    "#from networkx.algorithms import isomorphism\n",
    "#GM = isomorphism.GraphMatcher(create_graph(text1),create_graph(text))\n",
    "#print(GM.subgraph_is_isomorphic())\n",
    "#GM.mapping\n",
    "\n",
    "\n",
    "\n",
    "def Similar(text,text1):\n",
    "    \n",
    "    \n",
    "    G1=create_graph(text)\n",
    "    G2=create_graph(text1)\n",
    "    matching_graph=nx.Graph()\n",
    "\n",
    "    for n1,n2 in G2.edges():\n",
    "        if G1.has_edge(n1, n2):\n",
    "            matching_graph.add_edge(n1, n2)\n",
    "\n",
    "    components = nx.connected_components(matching_graph)\n",
    "\n",
    "    largest_component = max(components, key=len) \n",
    "    largest_common_subgraph = list( nx.induced_subgraph(matching_graph, largest_component))\n",
    "    \n",
    "    rslt = str(largest_common_subgraph)\n",
    "    x = rslt.split(\", \")\n",
    "\n",
    "    df = pd.DataFrame(x, columns = ['nodes']) \n",
    "    #print(df)\n",
    "    index = df.index\n",
    "    z = len(index)\n",
    "    if z==1 :  \n",
    "        a = len(index)-1\n",
    "    \n",
    "    else:\n",
    "        a = len(index)\n",
    "    \n",
    "\n",
    "\n",
    "    dist = 1- a/ max(create_graph(text).number_of_nodes(),create_graph(text1).number_of_nodes())\n",
    "    return dist\n",
    "\n",
    "print(Similar(text,text1))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import isomorphism\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from random import random\n",
    "from networkx.algorithms import isomorphism\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "#import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "import trafilatura\n",
    "import sys \n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms import isomorphism\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from random import random\n",
    "from networkx.algorithms import isomorphism\n",
    "import pandas as pd\n",
    "import trafilatura\n",
    "import sys \n",
    "import os\n",
    "\n",
    "\n",
    "def load_file(filename):    \n",
    "    with open(filename,'r') as f:\n",
    "        text = f.readlines()\n",
    "    return text\n",
    "\n",
    "def preprocess(line):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    line = [item.lower() for item in line if not item.lower() in stop_words]\n",
    "    return line\n",
    "\n",
    "def create_graph(text):\n",
    "    \n",
    "    word_list = []\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for line in text:\n",
    "        line = (line.strip()).split()\n",
    "        line = preprocess(line)\n",
    "        for i, word in enumerate(line):\n",
    "            if i != len(line)-1:\n",
    "                word_a = word\n",
    "                word_b = line[i+1]\n",
    "                if word_a not in word_list:\n",
    "                    word_list.append(word_a)\n",
    "                if word_b not in word_list:\n",
    "                    word_list.append(word_b)\n",
    "                if G.has_edge(word_a,word_b):\n",
    "                    G[word_a][word_b]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(word_a,word_b, weight = 1)\n",
    "  \n",
    "    return G        \n",
    "  \n",
    "def similar(text,text1):  \n",
    "    G1=create_graph(text)\n",
    "    G2=create_graph(text1)\n",
    "    matching_graph=nx.Graph()\n",
    "\n",
    "    for n1,n2 in G2.edges():\n",
    "        if G1.has_edge(n1, n2):\n",
    "            matching_graph.add_edge(n1, n2)\n",
    "\n",
    "    components = nx.connected_components(matching_graph)\n",
    "\n",
    "    largest_component = max(components, key=len) \n",
    "    largest_common_subgraph = list( nx.induced_subgraph(matching_graph, largest_component))\n",
    "    \n",
    "    rslt = str(largest_common_subgraph)\n",
    "    x = rslt.split(\", \")\n",
    "\n",
    "    df = pd.DataFrame(x, columns = ['nodes']) \n",
    "    #print(df)\n",
    "    index = df.index\n",
    "    z = len(index)\n",
    "    if z==1 :  \n",
    "        a = len(index)-1\n",
    "    \n",
    "    else:\n",
    "        a = len(index)\n",
    "    #print(a)\n",
    "\n",
    "\n",
    "    dist = 1- a/ max(create_graph(text).number_of_nodes(),create_graph(text1).number_of_nodes())\n",
    "    return dist\n",
    "\n",
    "#erase pre exist files and save it alwyas to sample.txt\n",
    "def content_extractor(url):\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        a1 = str(trafilatura.core.extract(downloaded)) \n",
    "        a2=text_lowercase(a1)\n",
    "        a3=denoise_text(a2)\n",
    "        a4 = nltk.word_tokenize(a3)\n",
    "        a5 = normalize(a4)\n",
    "        a= stem_and_lemmatize(a5) \n",
    "        text_file = open(\"sample.txt\", \"w\")\n",
    "        n = text_file.write(str(a))\n",
    "        text_file.close()\n",
    "        \n",
    "\n",
    "        \n",
    "#create a file with the given name and return the name as output\n",
    "\"\"\"def content_extract(url,filename):\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        a = trafilatura.core.extract(downloaded) \n",
    "        b= str(filename)\n",
    "        c= b+\".txt\"\n",
    "        text_file = open(c, \"w\")\n",
    "        text_file.write(str(a))\n",
    "        text_file.close()\n",
    "        base=os.path.basename('Downloads/Compressed/test/'+c)\n",
    "        return    base\"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "#######debut preprocessing\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def text_lowercase(text): \n",
    "    return text.lower() \n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    #words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def stem_and_lemmatize(words):   \n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return  lemmas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def content_extract(url,filename):\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        a1 = str(trafilatura.core.extract(downloaded)) \n",
    "        b= str(filename)\n",
    "        c= b+\".txt\"\n",
    "        a2=text_lowercase(a1)\n",
    "        a3=denoise_text(a2)\n",
    "        a4 = nltk.word_tokenize(a3)\n",
    "        a5 = normalize(a4)\n",
    "        a= stem_and_lemmatize(a5)   \n",
    "        text_file = open(c, \"w\")\n",
    "        text_file.write(str(a))   \n",
    "        text_file.close()\n",
    "        base=os.path.basename('Downloads/Compressed/test/'+c)\n",
    "        return    base\n",
    "####### fin preprocessing      \n",
    "                              \n",
    "    \n",
    "#extract and create our set of text from google search url result     \n",
    "def graph_init():\n",
    "    i=0\n",
    "    txt = open(\"tes.txt\", \"r\")\n",
    "    s=txt.read().splitlines()\n",
    "    num_lines = sum(1 for line in open('tes.txt'))\n",
    "    for x in s:\n",
    "            content_extract(x,\"filename\"+str(i))\n",
    "            i=i+1\n",
    "\n",
    "def test(filename): \n",
    "    chaine = \"['none']\"\n",
    "    a=str(filename)\n",
    "    fichier = open(a+\".txt\",\"r\")\n",
    "    for ligne in fichier:       \n",
    "        if chaine in ligne:           \n",
    "            fichier.close()\n",
    "            b=0\n",
    "        else:\n",
    "            b=1\n",
    "        return b\n",
    "\n",
    "def sim_moyen(url):\n",
    "    content_extractor(url)\n",
    "    graph_init()\n",
    "    text0=load_file('sample.txt')    \n",
    "    text = load_file('filename0.txt')       \n",
    "    text1 = load_file('filename1.txt')\n",
    "    \n",
    "    if test('sample')==0:\n",
    "        moyen=test('sample')+1\n",
    "    else:\n",
    "        moyen=(similar(text0,text)+similar(text0,text1))/2    \n",
    "    return moyen\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#test\n",
    "#text = load_file('filename0.txt')       \n",
    "#text1 = load_file('filename1.txt')   \n",
    "\n",
    "print(sim_moyen('https://www.knowbe4.com/phishing'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5011877556826512\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial import distance\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "def vector(filename):   \n",
    "    b= str(filename)\n",
    "    c= b+\".txt\"   \n",
    "    speech2 = open(c, \"r\")\n",
    "    corpus = [speech2.read()]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(corpus)\n",
    "    skl_output = vectorizer.transform(corpus)\n",
    "#rint(vectorizer.get_feature_names())\n",
    "#print(vectorizer.idf_)\n",
    "#print (skl_output.shape)\n",
    "    vec=skl_output[0].toarray()\n",
    "    vect=vec[0]\n",
    "    return vec \n",
    "vect2=vector('filename0')\n",
    "vect1=vector('filename1')\n",
    "vect=vector('sample')\n",
    "vec2=(vect2[0]).tolist()\n",
    "vec1=(vect1[0]).tolist()\n",
    "vec=(vect[0]).tolist()\n",
    "\n",
    "\n",
    "\n",
    "def sim_cosine(vec1,vec2):\n",
    "    \n",
    "    if len(vec1)>len(vec2):\n",
    "        a=len(vec1)-len(vec2)\n",
    "        for i in range(a):\n",
    "            vec2.append(0)\n",
    "    else:\n",
    "        a=len(vec2)-len(vec1)\n",
    "        for i in range(a):\n",
    "            vec1.append(0)\n",
    "\n",
    "    return dot(vec1, vec2)/(norm(vec1)*norm(vec2))\n",
    "\n",
    "#def jaccard_sim(list1, list2):\n",
    "    #intersection = len(list(set(list1).intersection(list2)))\n",
    "    #union = (len(list1) + len(list2)) - intersection\n",
    "    #return float(intersection) / union   \n",
    "\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    s1 = set(list1)\n",
    "    s2 = set(list2)\n",
    "    return len(s1.intersection(s2)) / len(s1.union(s2))\n",
    "\n",
    "a=sim_cosine(vec,vec1)\n",
    "a1=sim_cosine(vec,vec2)\n",
    "b=jaccard_similarity(vec,vec1)\n",
    "b1=jaccard_similarity(vec,vec1)\n",
    "#c = distance.euclidean(vec, vec1)\n",
    "\n",
    "print((a1+a)/2)\n",
    "print(b1)\n",
    "print(b)\n",
    "#print(1 / (1 + c**(0.25)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import isomorphism\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from random import random\n",
    "from networkx.algorithms import isomorphism\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "#import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "import trafilatura\n",
    "import sys \n",
    "import os\n",
    "\n",
    "\n",
    "#######debut preprocessing\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def text_lowercase(text): \n",
    "    return text.lower() \n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def stem_and_lemmatize(words):   \n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return  lemmas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def content_extract(url,filename):\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        a1 = str(trafilatura.core.extract(downloaded)) \n",
    "        b= str(filename)\n",
    "        c= b+\".txt\"\n",
    "        a2=text_lowercase(a1)\n",
    "        a3=denoise_text(a2)\n",
    "        a4 = nltk.word_tokenize(a3)\n",
    "        a5 = normalize(a4)\n",
    "        a= stem_and_lemmatize(a5)   \n",
    "        text_file = open(c, \"w\")\n",
    "        text_file.write(str(a))  \n",
    "        \n",
    "        text_file.close()\n",
    "####### fin preprocessing        \n",
    "\n",
    "        \n",
    "def graph_init():\n",
    "    i=0\n",
    "    txt = open(\"tes.txt\", \"r\")\n",
    "    s=txt.read().splitlines()\n",
    "    num_lines = sum(1 for line in open('tes.txt'))\n",
    "    #tab=['https://www.fondation-louisbonduelle.org/category/education/','https://www.fondation-louisbonduelle.org/category/chronique-dexperts/']\n",
    "    for x in s:\n",
    "            print(i)\n",
    "            content_extract(x,\"filename\"+str(i))\n",
    "            i=i+1\n",
    "#graph_init()\n",
    "\n",
    "\n",
    "#import textcleaner as tc\n",
    "#a=tc.main_cleaner('sample.txt')\n",
    "#print(a)\n",
    "\n",
    "content_extract('https://www.consumer.ftc.gov/articles/how-recognize-and-avoid-phishing-scams','filename')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"import re\n",
    "string = open('filename.txt').read()\n",
    "new_str = re.sub('[^a-zA-Z0-9\\n\\.]', ' ', string)\n",
    "open('filename.txt', 'w').write(new_str)\"\"\"\n",
    "\n",
    "\"\"\"chaine = \"None\" # Texte à rechercher\n",
    "text0=load_file('sample.txt') \n",
    "fichier = open(\"filename.txt\",\"r\")\n",
    "for ligne in fichier:\n",
    "    if chaine in ligne:\n",
    "        print (ligne)\n",
    "fichier.close()\"\"\"\n",
    "\n",
    "def test(filename): \n",
    "    chaine = \"None\"\n",
    "    a=str(filename)\n",
    "    fichier = open(a+\".txt\",\"r\")\n",
    "    for ligne in fichier:       \n",
    "        if chaine in ligne:           \n",
    "            fichier.close()\n",
    "            b=1\n",
    "        else:\n",
    "            b=0\n",
    "        return b\n",
    "print (test('sample'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"try: \n",
    "    from googlesearch import search \n",
    "except ImportError: \n",
    "    print(\"No module named 'google' found\") \n",
    "\n",
    "# to search \n",
    "query = \"phishing\"\n",
    "tab=[]\n",
    "for j in search(query, tld=\"com\", num=2, stop=2, pause=10): \n",
    "    tab.append(j)\n",
    "print(tab)\n",
    "with open('tes.txt', 'w') as f:\n",
    "    for item in tab:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "        \"\"\"\n",
    "from collections import deque\n",
    "from urllib.parse import urljoin\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import networkx as nx\n",
    "from networkx.algorithms import isomorphism\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from random import random\n",
    "from networkx.algorithms import isomorphism\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from networkx.algorithms import isomorphism\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from random import random\n",
    "import pandas as pd\n",
    "import trafilatura\n",
    "import sys \n",
    "import os        \n",
    "\n",
    "\n",
    "\n",
    "###preprocess\n",
    "\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def text_lowercase(text): \n",
    "    return text.lower() \n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    #words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def stem_and_lemmatize(words):   \n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return  lemmas\n",
    "\n",
    "####preprocess\n",
    "\n",
    "\n",
    "def content_extract(url,filename):\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        a1 = str(trafilatura.core.extract(downloaded)) \n",
    "        b= str(filename)\n",
    "        c= b+\".txt\"\n",
    "        a2=text_lowercase(a1)\n",
    "        a3=denoise_text(a2)\n",
    "        a4 = nltk.word_tokenize(a3)\n",
    "        a5 = normalize(a4)\n",
    "        a= stem_and_lemmatize(a5)   \n",
    "        text_file = open(c, \"w\")\n",
    "        text_file.write(str(a))   \n",
    "        text_file.close()\n",
    "        base=os.path.basename('Downloads/Compressed/test/'+c)\n",
    "        return    base        \n",
    "      \n",
    "        \n",
    "def graph_init():\n",
    "    i=0\n",
    "    j=1\n",
    "    txt = open(\"tes.txt\", \"r\")\n",
    "    s=txt.read().splitlines()\n",
    "    num_lines = sum(1 for line in open('tes.txt'))\n",
    "    for x in s:\n",
    "            content_extract(x,\"filename\"+str(i))\n",
    "            i=i+1\n",
    "graph_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Page to be crawled: https://www.phishing.org/\n",
      "===================\n",
      "\n",
      "\n",
      "==============\n",
      "Pages crawled:\n",
      "==============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'for i in visited_list:\\n    print(sim_moyen(i))\\n    if sim_moyen(i) > 0.99: \\n        \\n        print (i)\\n        '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import deque\n",
    "from urllib.parse import urljoin\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import networkx as nx\n",
    "from networkx.algorithms import isomorphism\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from random import random\n",
    "from networkx.algorithms import isomorphism\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "#import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from networkx.algorithms import isomorphism\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from random import random\n",
    "import pandas as pd\n",
    "import trafilatura\n",
    "import sys \n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###Module isomorphisme Debut\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_file(filename):    \n",
    "    with open(filename,'r') as f:\n",
    "        text = f.readlines()\n",
    "    return text\n",
    "\n",
    "def preprocess(line):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    line = [item.lower() for item in line if not item.lower() in stop_words]\n",
    "    return line\n",
    "\n",
    "def create_graph(text):\n",
    "    \n",
    "    word_list = []\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for line in text:\n",
    "        line = (line.strip()).split()\n",
    "        line = preprocess(line)\n",
    "        for i, word in enumerate(line):\n",
    "            if i != len(line)-1:\n",
    "                word_a = word\n",
    "                word_b = line[i+1]\n",
    "                if word_a not in word_list:\n",
    "                    word_list.append(word_a)\n",
    "                if word_b not in word_list:\n",
    "                    word_list.append(word_b)\n",
    "                if G.has_edge(word_a,word_b):\n",
    "                    G[word_a][word_b]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(word_a,word_b, weight = 1)\n",
    "  \n",
    "    return G        \n",
    "  \n",
    "def similar(text,text1):  \n",
    "    G1=create_graph(text)\n",
    "    G2=create_graph(text1)\n",
    "    matching_graph=nx.Graph()\n",
    "\n",
    "    for n1,n2 in G2.edges():\n",
    "        if G1.has_edge(n1, n2):\n",
    "            matching_graph.add_edge(n1, n2)\n",
    "\n",
    "    components = nx.connected_components(matching_graph)\n",
    "\n",
    "    largest_component = max(components, key=len) \n",
    "    largest_common_subgraph = list( nx.induced_subgraph(matching_graph, largest_component))\n",
    "    \n",
    "    rslt = str(largest_common_subgraph)\n",
    "    x = rslt.split(\", \")\n",
    "\n",
    "    df = pd.DataFrame(x, columns = ['nodes']) \n",
    "    #print(df)\n",
    "    index = df.index\n",
    "    z = len(index)\n",
    "    if z==1 :  \n",
    "        a = len(index)-1\n",
    "    \n",
    "    else:\n",
    "        a = len(index)\n",
    "    #print(a)\n",
    "\n",
    "    dist = 1- a/ max(create_graph(text).number_of_nodes(),create_graph(text1).number_of_nodes()) \n",
    "    #dist = a/ max(create_graph(text).number_of_nodes(),create_graph(text1).number_of_nodes())\n",
    "    \n",
    "    \n",
    "    return dist\n",
    "\n",
    "#erase pre exist files and save it alwyas to sample.txt\n",
    "def content_extractor(url):\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        a1 = str(trafilatura.core.extract(downloaded)) \n",
    "        a2=text_lowercase(a1)\n",
    "        a3=denoise_text(a2)\n",
    "        a4 = nltk.word_tokenize(a3)\n",
    "        a5 = normalize(a4)\n",
    "        a= stem_and_lemmatize(a5) \n",
    "        text_file = open(\"sample.txt\", \"w\")\n",
    "        n = text_file.write(str(a))\n",
    "        text_file.close()\n",
    "        \n",
    "\n",
    "        \n",
    "#create a file with the given name and return the name as output\n",
    "\"\"\"def content_extract(url,filename):\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        a = trafilatura.core.extract(downloaded) \n",
    "        b= str(filename)\n",
    "        c= b+\".txt\"\n",
    "        text_file = open(c, \"w\")\n",
    "        text_file.write(str(a))\n",
    "        text_file.close()\n",
    "        base=os.path.basename('Downloads/Compressed/test/'+c)\n",
    "        return    base\"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "#######debut preprocessing\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def text_lowercase(text): \n",
    "    return text.lower() \n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    #words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def stem_and_lemmatize(words):   \n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return  lemmas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def content_extract(url,filename):\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        a1 = str(trafilatura.core.extract(downloaded)) \n",
    "        b= str(filename)\n",
    "        c= b+\".txt\"\n",
    "        a2=text_lowercase(a1)\n",
    "        a3=denoise_text(a2)\n",
    "        a4 = nltk.word_tokenize(a3)\n",
    "        a5 = normalize(a4)\n",
    "        a= stem_and_lemmatize(a5)   \n",
    "        text_file = open(c, \"w\")\n",
    "        text_file.write(str(a))   \n",
    "        text_file.close()\n",
    "        base=os.path.basename('Downloads/Compressed/test/'+c)\n",
    "        return    base\n",
    "####### fin preprocessing      \n",
    "                              \n",
    "    \n",
    "#extract and create our set of text from google search url result     \n",
    "def graph_init():\n",
    "    i=0\n",
    "    txt = open(\"tes.txt\", \"r\")\n",
    "    s=txt.read().splitlines()\n",
    "    num_lines = sum(1 for line in open('tes.txt'))\n",
    "    for x in s:\n",
    "            content_extract(x,\"filename\"+str(i))\n",
    "            i=i+1\n",
    "            \n",
    "\n",
    "\n",
    "def test(filename): \n",
    "    chaine = \"['none']\"\n",
    "    a=str(filename)\n",
    "    fichier = open(a+\".txt\",\"r\")\n",
    "    for ligne in fichier:       \n",
    "        if chaine in ligne:           \n",
    "            fichier.close()\n",
    "            b=0\n",
    "        else:\n",
    "            b=1\n",
    "        return b\n",
    "\n",
    "    \n",
    "\n",
    "def search(word):\n",
    "    file = open(\"sample.txt\", \"r\")\n",
    "    data = file.read()\n",
    "    words=data.split()\n",
    "    found = re.findall('\\\\b' + word + '\\\\b', data)\n",
    "    if found:\n",
    "        #print(True, '{word} occurs {counts} time'.format(word=word, counts=found.count(word)))\n",
    "        counts=found.count(word)\n",
    "        tf=counts/len(words)\n",
    "    else:\n",
    "        counts=0\n",
    "        tf=0\n",
    "    return tf\n",
    "    \n",
    "def sim_moyen(url):\n",
    "    content_extractor(url)\n",
    "    #graph_init()\n",
    "    text0=load_file('sample.txt')    \n",
    "    text = load_file('filename0.txt')       \n",
    "    text1 = load_file('filename1.txt') \n",
    "    \n",
    "    try:\n",
    "        if test('sample')==0:\n",
    "            moyen=test('sample')+1\n",
    "        else:\n",
    "            sim1=similar(text0,text)\n",
    "            sim2=similar(text0,text1)\n",
    "            moyen=(sim1+sim2)/2 \n",
    "    except ValueError as ve:\n",
    "        moyen=1                      #when not common subgraph exist betwenn G1 annd G2\n",
    "    return moyen\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "####Module isomorphisme Fin\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read URL from command line\n",
    "url = \"https://www.phishing.org/\"\n",
    "\n",
    "print (\"===================\")\n",
    "print (\"Page to be crawled:\", url)\n",
    "print( \"===================\")\n",
    "print()\n",
    "\n",
    "# Create queue\n",
    "queue = []\n",
    "\n",
    "# Maintains list of visited pages\n",
    "visited_list = []\n",
    "# Crawl the page and populate the queue with newly found URLs\n",
    "def crawl(url):\n",
    "    visited_list.append((sim_moyen(url),url))\n",
    "    if len(queue) > 3:\n",
    "        return\n",
    "    \n",
    "    urlf = urlopen(url)\n",
    "    soup = BeautifulSoup(urlf.read())\n",
    "    urls = soup.findAll(\"a\", href=True)\n",
    "\n",
    "    for i in urls:\n",
    "        \n",
    "        flag = 0\n",
    "        # Complete relative URLs and strip trailing slash\n",
    "        complete_url = urljoin(url, i[\"href\"]).rstrip('/')\n",
    "        \n",
    "        # Check if the URL already exists in the queue\n",
    "        for j in queue:\n",
    "            if j == complete_url:\n",
    "                flag = 1\n",
    "                break\n",
    "\n",
    "        # If not found in queue\n",
    "        if flag == 0 :\n",
    "            if len(queue) > 3:\n",
    "                \n",
    "                return\n",
    "            if ((visited_list.count(complete_url)) == 0 and sim_moyen(complete_url)<1)  :  # a modifier \n",
    "                queue.append((sim_moyen(complete_url),complete_url))\n",
    "                \n",
    "\n",
    "    # sort the queue and Pop one URL from the queue so that it can be crawled\n",
    "    queue.sort()\n",
    "    current = queue.pop(0)\n",
    "    \n",
    "    # Recursive call to crawl until the queue is populated with 100 URLs\n",
    "    crawl(current)\n",
    "\n",
    "crawl(url)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"for i in queue:\n",
    "    print(sim_moyen(i)) \n",
    "    print (i)\"\"\"\n",
    "\n",
    "print()\n",
    "print (\"==============\")\n",
    "print (\"Pages crawled:\")\n",
    "print (\"==============\")\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "# Print list of visited pages\n",
    "\"\"\"for i in visited_list:\n",
    "    print(sim_moyen(i))\n",
    "    if sim_moyen(i) > 0.99: \n",
    "        \n",
    "        print (i)\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9823443223443223, 'https://www.phishing.org/how-to-report-phishing'), (0.9822812192723698, 'https://www.phishing.org/about-us'), (1, 'https://www.phishing.org/?hsLang=en'), (0.9844290657439446, 'https://www.phishing.org/what-is-phishing')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(queue)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.055666003976143144\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def load_file(filename):    \n",
    "    with open(filename,'r') as f:\n",
    "        text = f.readlines()\n",
    "    return text\n",
    "\n",
    "def search(word):\n",
    "    file = open(\"filename.txt\", \"r\")\n",
    "    data = file.read()\n",
    "    words=data.split()\n",
    "    found = re.findall('\\\\b' + word + '\\\\b', data)\n",
    "    if found:\n",
    "        #print(True, '{word} occurs {counts} time'.format(word=word, counts=found.count(word)))\n",
    "        counts=found.count(word)\n",
    "        tf=counts/len(words)\n",
    "    else:\n",
    "        counts=0\n",
    "        tf=0\n",
    "    return tf\n",
    "\n",
    "print(search('phishing'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    s1 = set(list1)\n",
    "    s2 = set(list2)\n",
    "    return len(s1.intersection(s2)) / len(s1.union(s2))\n",
    "list1 = ['dog', 'cat', 'cat', 'rat']\n",
    "list2 = ['dog', 'cat', 'mouse']\n",
    "print (jaccard_similarity(list1, list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"try: \n",
    "    from googlesearch import search \n",
    "except ImportError: \n",
    "    print(\"No module named 'google' found\") \n",
    "\n",
    "# to search \n",
    "query = \"scam\"\n",
    "tab=[]\n",
    "for j in search(query, tld=\"com\", num=2, stop=2, pause=10): \n",
    "    tab.append(j)\n",
    "print(tab)\n",
    "with open('tes.txt', 'w') as f:\n",
    "    for item in tab:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "        \"\"\"\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial import distance\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from urllib.parse import urljoin\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import networkx as nx\n",
    "from networkx.algorithms import isomorphism\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from random import random\n",
    "from networkx.algorithms import isomorphism\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from networkx.algorithms import isomorphism\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from random import random\n",
    "import pandas as pd\n",
    "import trafilatura\n",
    "import sys \n",
    "import os        \n",
    "\n",
    "\n",
    "\n",
    "###preprocess\n",
    "\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def text_lowercase(text): \n",
    "    return text.lower() \n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    #words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def stem_and_lemmatize(words):   \n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return  lemmas\n",
    "\n",
    "####preprocess\n",
    "\n",
    "\n",
    "def content_extract(url,filename):\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        a1 = str(trafilatura.core.extract(downloaded)) \n",
    "        b= str(filename)\n",
    "        c= b+\".txt\"\n",
    "        a2=text_lowercase(a1)\n",
    "        a3=denoise_text(a2)\n",
    "        a4 = nltk.word_tokenize(a3)\n",
    "        a5 = normalize(a4)\n",
    "        a= stem_and_lemmatize(a5)   \n",
    "        text_file = open(c, \"w\")\n",
    "        text_file.write(str(a))   \n",
    "        text_file.close()\n",
    "        base=os.path.basename('Downloads/Compressed/test/'+c)\n",
    "        return    base        \n",
    "      \n",
    "        \n",
    "def graph_init():\n",
    "    i=0\n",
    "    j=1\n",
    "    txt = open(\"tes.txt\", \"r\")\n",
    "    s=txt.read().splitlines()\n",
    "    num_lines = sum(1 for line in open('tes.txt'))\n",
    "    for x in s:\n",
    "            content_extract(x,\"filename\"+str(i))\n",
    "            i=i+1\n",
    "graph_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Page to be crawled: https://www.phishing.org/\n",
      "===================\n",
      "\n",
      "\n",
      "==============\n",
      "Pages crawled:\n",
      "==============\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'for i in visited_list:\\n    print(sim_moyen(i))\\n    if sim_moyen(i) > 0.99: \\n        \\n        print (i)\\n        '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial import distance\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from urllib.parse import urljoin\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import networkx as nx\n",
    "from networkx.algorithms import isomorphism\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from random import random\n",
    "from networkx.algorithms import isomorphism\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "#import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from networkx.algorithms import isomorphism\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from random import random\n",
    "import pandas as pd\n",
    "import trafilatura\n",
    "import sys \n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###Module isomorphisme Debut\n",
    "\n",
    "\n",
    "def preprocess(line):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    line = [item.lower() for item in line if not item.lower() in stop_words]\n",
    "    return line\n",
    "\n",
    " \n",
    "def vector(filename):   \n",
    "    b= str(filename)\n",
    "    c= b+\".txt\"   \n",
    "    speech2 = open(c, \"r\")\n",
    "    corpus = [speech2.read()]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(corpus)\n",
    "    skl_output = vectorizer.transform(corpus)\n",
    "    vec=skl_output[0].toarray()\n",
    "    vect=vec[0]\n",
    "    return vec\n",
    "\n",
    "def sim_cosine(vec1,vec2):\n",
    "    \n",
    "    if len(vec1)>len(vec2):\n",
    "        a=len(vec1)-len(vec2)\n",
    "        for i in range(a):\n",
    "            vec2.append(0)\n",
    "    else:\n",
    "        a=len(vec2)-len(vec1)\n",
    "        for i in range(a):\n",
    "            vec1.append(0)\n",
    "\n",
    "    return dot(vec1, vec2)/(norm(vec1)*norm(vec2))\n",
    "\n",
    "\n",
    "\n",
    "#erase pre exist files and save it alwyas to sample.txt\n",
    "def content_extractor(url):\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        a1 = str(trafilatura.core.extract(downloaded)) \n",
    "        a2=text_lowercase(a1)\n",
    "        a3=denoise_text(a2)\n",
    "        a4 = nltk.word_tokenize(a3)\n",
    "        a5 = normalize(a4)\n",
    "        a= stem_and_lemmatize(a5) \n",
    "        text_file = open(\"sample.txt\", \"w\")\n",
    "        n = text_file.write(str(a))\n",
    "        text_file.close()\n",
    "        \n",
    "\n",
    "#######debut preprocessing\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def text_lowercase(text): \n",
    "    return text.lower() \n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    #words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def stem_and_lemmatize(words):   \n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return  lemmas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def content_extract(url,filename):\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        a1 = str(trafilatura.core.extract(downloaded)) \n",
    "        b= str(filename)\n",
    "        c= b+\".txt\"\n",
    "        a2=text_lowercase(a1)\n",
    "        a3=denoise_text(a2)\n",
    "        a4 = nltk.word_tokenize(a3)\n",
    "        a5 = normalize(a4)\n",
    "        a= stem_and_lemmatize(a5)   \n",
    "        text_file = open(c, \"w\")\n",
    "        text_file.write(str(a))   \n",
    "        text_file.close()\n",
    "        base=os.path.basename('Downloads/Compressed/test/'+c)\n",
    "        return    base\n",
    "####### fin preprocessing      \n",
    "                              \n",
    "    \n",
    "#extract and create our set of text from google search url result     \n",
    "def graph_init():\n",
    "    i=0\n",
    "    txt = open(\"tes.txt\", \"r\")\n",
    "    s=txt.read().splitlines()\n",
    "    num_lines = sum(1 for line in open('tes.txt'))\n",
    "    for x in s:\n",
    "            content_extract(x,\"filename\"+str(i))\n",
    "            i=i+1\n",
    "            \n",
    "def test(filename): \n",
    "    chaine = \"['none']\"\n",
    "    a=str(filename)\n",
    "    fichier = open(a+\".txt\",\"r\")\n",
    "    for ligne in fichier:       \n",
    "        if chaine in ligne:           \n",
    "            fichier.close()\n",
    "            b=0\n",
    "        else:\n",
    "            b=1\n",
    "        return b\n",
    "\n",
    "\n",
    "def sim_moyen(url):   \n",
    "    content_extractor(url)\n",
    "    vect2=vector('filename0')\n",
    "    vect1=vector('filename1')\n",
    "    vect=vector('sample')\n",
    "    vec2=(vect2[0]).tolist()\n",
    "    vec1=(vect1[0]).tolist()\n",
    "    vec=(vect[0]).tolist()\n",
    "    try:\n",
    "        if test('sample')==0:\n",
    "            moyen=test('sample')\n",
    "        else:\n",
    "            sim1=sim_cosine(vec,vec1)\n",
    "            sim2=sim_cosine(vec,vec2)\n",
    "            moyen=((sim1+sim2)/2) \n",
    "    except ValueError as ve:\n",
    "        moyen=0                       #when a vector is empty\n",
    "    return moyen\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####Module isomorphisme Fin\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read URL from command line\n",
    "url = \"https://www.phishing.org/\"\n",
    "\n",
    "print (\"===================\")\n",
    "print (\"Page to be crawled:\", url)\n",
    "print( \"===================\")\n",
    "print()\n",
    "\n",
    "# Create queue\n",
    "queue = []\n",
    "\n",
    "# Maintains list of visited pages\n",
    "visited_list = []\n",
    "# Crawl the page and populate the queue with newly found URLs\n",
    "def crawl(url):\n",
    "    visited_list.append((sim_moyen(url),url))\n",
    "    if len(queue) > 15:\n",
    "        return\n",
    "    \n",
    "    urlf = urlopen(url)\n",
    "    soup = BeautifulSoup(urlf.read())\n",
    "    urls = soup.findAll(\"a\", href=True)\n",
    "\n",
    "    for i in urls:\n",
    "        \n",
    "        flag = 0\n",
    "        # Complete relative URLs and strip trailing slash\n",
    "        complete_url = urljoin(url, i[\"href\"]).rstrip('/')\n",
    "        \n",
    "        # Check if the URL already exists in the queue\n",
    "        for j in queue:\n",
    "            if j == complete_url:\n",
    "                flag = 1\n",
    "                break\n",
    "\n",
    "        # If not found in queue\n",
    "        if flag == 0 :\n",
    "            if len(queue) > 15:\n",
    "                \n",
    "                return\n",
    "            if ((visited_list.count(complete_url)) == 0 and sim_moyen(complete_url)<1)  :  # a modifier \n",
    "                queue.append((sim_moyen(complete_url),complete_url))\n",
    "                \n",
    "\n",
    "    # sort the queue and Pop one URL from the queue so that it can be crawled\n",
    "    queue.sort()\n",
    "    current = queue.pop()[1]\n",
    "    \n",
    "    # Recursive call to crawl until the queue is populated with 100 URLs\n",
    "    crawl(current)\n",
    "\n",
    "crawl(url)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"for i in queue:\n",
    "    print(sim_moyen(i)) \n",
    "    print (i)\"\"\"\n",
    "\n",
    "print()\n",
    "print (\"==============\")\n",
    "print (\"Pages crawled:\")\n",
    "print (\"==============\")\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "# Print list of visited pages\n",
    "\"\"\"for i in visited_list:\n",
    "    print(sim_moyen(i))\n",
    "    if sim_moyen(i) > 0.99: \n",
    "        \n",
    "        print (i)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'https://www.phishing.org/how-to-report-phishing'), (0.4013751389612935, 'https://www.phishing.org/about-us'), (0.38788948542108853, 'https://www.phishing.org/phishing-security-test'), (0.32677158918322774, 'https://www.phishing.org/?hsLang=en'), (0.32677158918322774, 'https://www.phishing.org'), (0, 'https://www.phishing.org/what-is-phishing'), (0.21928576189956905, 'https://www.phishing.org/history-of-phishing'), (0.34341423948063077, 'https://www.phishing.org/phishing-techniques'), (0.32677158918322774, 'https://www.phishing.org'), (0.23057036261417235, 'https://www.phishing.org/common-phishing-scams'), (0.33259955796129714, 'https://www.phishing.org/phishing-examples'), (0.3098992820989574, 'https://www.phishing.org/phishing-and-spoofing'), (0.37946385342848016, 'https://www.phishing.org/phishing-and-identity-theft'), (0.32677158918322774, 'https://www.phishing.org'), (0.30767192192719317, 'https://www.phishing.org/10-ways-to-avoid-phishing-scams'), (0.4057165133342048, 'https://www.phishing.org/how-to-phish-employees')]\n"
     ]
    }
   ],
   "source": [
    "print(queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.consumer.ftc.gov/features/scam-alerts', 'https://en.wikipedia.org/wiki/Advance-fee_scam']\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    from googlesearch import search \n",
    "except ImportError: \n",
    "    print(\"No module named 'google' found\") \n",
    "\n",
    "# to search \n",
    "query = \"scam\"\n",
    "tab=[]\n",
    "for j in search(query, tld=\"com\", num=2, stop=2, pause=10): \n",
    "    tab.append(j)\n",
    "print(tab)\n",
    "with open('tes.txt', 'w') as f:\n",
    "    for item in tab:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
