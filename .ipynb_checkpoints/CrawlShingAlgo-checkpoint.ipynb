{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from urllib.parse import urljoin\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import networkx as nx\n",
    "from networkx.algorithms import isomorphism\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from random import random\n",
    "from networkx.algorithms import isomorphism\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from networkx.algorithms import isomorphism\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from random import random\n",
    "import pandas as pd\n",
    "import trafilatura\n",
    "import sys \n",
    "import os        \n",
    "\n",
    "\n",
    "\n",
    "###preprocess\n",
    "\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def text_lowercase(text): \n",
    "    return text.lower() \n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    #words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def stem_and_lemmatize(words):   \n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return  lemmas\n",
    "\n",
    "####preprocess\n",
    "\n",
    "\n",
    "def content_extract(url,filename):\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        a1 = str(trafilatura.core.extract(downloaded)) \n",
    "        b= str(filename)\n",
    "        c= b+\".txt\"\n",
    "        a2=text_lowercase(a1)\n",
    "        a3=denoise_text(a2)\n",
    "        a4 = nltk.word_tokenize(a3)\n",
    "        a5 = normalize(a4)\n",
    "        a= stem_and_lemmatize(a5)   \n",
    "        text_file = open(c, \"w\")\n",
    "        text_file.write(str(a))   \n",
    "        text_file.close()\n",
    "        base=os.path.basename('Downloads/Compressed/test/'+c)\n",
    "        return    base        \n",
    "      \n",
    "        \n",
    "def graph_init():\n",
    "    i=0\n",
    "    j=1\n",
    "    txt = open(\"tes.txt\", \"r\")\n",
    "    s=txt.read().splitlines()\n",
    "    num_lines = sum(1 for line in open('tes.txt'))\n",
    "    for x in s:\n",
    "            content_extract(x,\"filename\"+str(i))\n",
    "            i=i+1\n",
    "graph_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import urllib.robotparser\n",
    "from urllib.parse import urljoin\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import networkx as nx\n",
    "from networkx.algorithms import isomorphism\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from random import random\n",
    "from networkx.algorithms import isomorphism\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "#import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from networkx.algorithms import isomorphism\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from random import random\n",
    "import pandas as pd\n",
    "import trafilatura\n",
    "import sys \n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###Module isomorphisme start\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_file(filename):    \n",
    "    with open(filename,'r') as f:\n",
    "        text = f.readlines()\n",
    "    return text\n",
    "\n",
    "def preprocess(line):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    line = [item.lower() for item in line if not item.lower() in stop_words]\n",
    "    return line\n",
    "\n",
    "def create_graph(text):\n",
    "    \n",
    "    word_list = []\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for line in text:\n",
    "        line = (line.strip()).split()\n",
    "        line = preprocess(line)\n",
    "        for i, word in enumerate(line):\n",
    "            if i != len(line)-1:\n",
    "                word_a = word\n",
    "                word_b = line[i+1]\n",
    "                if word_a not in word_list:\n",
    "                    word_list.append(word_a)\n",
    "                if word_b not in word_list:\n",
    "                    word_list.append(word_b)\n",
    "                if G.has_edge(word_a,word_b):\n",
    "                    G[word_a][word_b]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(word_a,word_b, weight = 1)\n",
    "  \n",
    "    return G        \n",
    "\n",
    "def search(word):\n",
    "    file = open(\"sample.txt\", \"r\")\n",
    "    data = file.read()\n",
    "    words=data.split()\n",
    "    found = re.findall('\\\\b' + word + '\\\\b', data)\n",
    "    if found:\n",
    "        #print(True, '{word} occurs {counts} time'.format(word=word, counts=found.count(word)))\n",
    "        counts=found.count(word)\n",
    "        tf=counts/len(words)\n",
    "    else:\n",
    "        counts=0\n",
    "        tf=0\n",
    "    return tf\n",
    "\n",
    "def similar(text,text1):  \n",
    "    G1=create_graph(text)\n",
    "    G2=create_graph(text1)\n",
    "    matching_graph=nx.Graph()\n",
    "\n",
    "    for n1,n2 in G2.edges():\n",
    "        if G1.has_edge(n1, n2):\n",
    "            matching_graph.add_edge(n1, n2)\n",
    "\n",
    "    components = nx.connected_components(matching_graph)\n",
    "\n",
    "    largest_component = max(components, key=len) \n",
    "    largest_common_subgraph = list( nx.induced_subgraph(matching_graph, largest_component))\n",
    "    \n",
    "    rslt = str(largest_common_subgraph)\n",
    "    x = rslt.split(\", \")\n",
    "\n",
    "    df = pd.DataFrame(x, columns = ['nodes']) \n",
    "    #print(df)\n",
    "    index = df.index\n",
    "    z = len(index)\n",
    "    if z==1 :  \n",
    "        a = len(index)-1\n",
    "    \n",
    "    else:\n",
    "        a = len(index)\n",
    "    #print(a)\n",
    "\n",
    "\n",
    "    dist = 1- a/ max(create_graph(text).number_of_nodes(),create_graph(text1).number_of_nodes())\n",
    "    \n",
    "  \n",
    "    return dist\n",
    "\n",
    "#erase pre exist files and save it alwyas to sample.txt\n",
    "def content_extractor(url):\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        a1 = str(trafilatura.core.extract(downloaded)) \n",
    "        a2=text_lowercase(a1)\n",
    "        a3=denoise_text(a2)\n",
    "        a4 = nltk.word_tokenize(a3)\n",
    "        a5 = normalize(a4)\n",
    "        a= stem_and_lemmatize(a5) \n",
    "        text_file = open(\"sample.txt\", \"w\")\n",
    "        n = text_file.write(str(a))\n",
    "        text_file.close()\n",
    "        \n",
    "\n",
    "        \n",
    "#create a file with the given name and return the name as output\n",
    "\"\"\"def content_extract(url,filename):\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        a = trafilatura.core.extract(downloaded) \n",
    "        b= str(filename)\n",
    "        c= b+\".txt\"\n",
    "        text_file = open(c, \"w\")\n",
    "        text_file.write(str(a))\n",
    "        text_file.close()\n",
    "        base=os.path.basename('Downloads/Compressed/test/'+c)\n",
    "        return    base\"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "#######preprocessing start\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def text_lowercase(text): \n",
    "    return text.lower() \n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    #words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def stem_and_lemmatize(words):   \n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return  lemmas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def content_extract(url,filename):\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        a1 = str(trafilatura.core.extract(downloaded)) \n",
    "        b= str(filename)\n",
    "        c= b+\".txt\"\n",
    "        a2=text_lowercase(a1)\n",
    "        a3=denoise_text(a2)\n",
    "        a4 = nltk.word_tokenize(a3)\n",
    "        a5 = normalize(a4)\n",
    "        a= stem_and_lemmatize(a5)   \n",
    "        text_file = open(c, \"w\")\n",
    "        text_file.write(str(a))   \n",
    "        text_file.close()\n",
    "        base=os.path.basename('Downloads/Compressed/test/'+c)\n",
    "        return    base\n",
    "#######  preprocessing  end    \n",
    "                              \n",
    "    \n",
    "#extract and create our set of text from google search url result     \n",
    "def graph_init():\n",
    "    i=0\n",
    "    txt = open(\"tes.txt\", \"r\")\n",
    "    s=txt.read().splitlines()\n",
    "    num_lines = sum(1 for line in open('tes.txt'))\n",
    "    for x in s:\n",
    "            content_extract(x,\"filename\"+str(i))\n",
    "            i=i+1\n",
    "            \n",
    "\n",
    "\n",
    "def test(filename): \n",
    "    chaine = \"['none']\"\n",
    "    a=str(filename)\n",
    "    fichier = open(a+\".txt\",\"r\")\n",
    "    for ligne in fichier:       \n",
    "        if chaine in ligne:           \n",
    "            fichier.close()\n",
    "            b=0\n",
    "        else:\n",
    "            b=1\n",
    "        return b\n",
    "    \n",
    "\n",
    "    \n",
    "def sim_moyen(url):\n",
    "    content_extractor(url)\n",
    "    #graph_init()\n",
    "    text0=load_file('sample.txt')    \n",
    "    text = load_file('filename0.txt')       \n",
    "    text1 = load_file('filename1.txt') \n",
    "    \n",
    "    try:\n",
    "        if test('sample')==0:\n",
    "            moyen=test('sample')+1\n",
    "        else:\n",
    "            sim1=similar(text0,text)-(search('phishing'))\n",
    "            sim2=similar(text0,text1)-(search('phishing'))\n",
    "            moyen=(sim1+sim2)/2 \n",
    "           # moyen=(similar(text0,text)+similar(text0,text1))/2 \n",
    "    except ValueError as ve:\n",
    "        moyen=1                      #when not common subgraph exist betwenn G1 annd G2\n",
    "    return moyen\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####Module isomorphisme end\n",
    "\n",
    "def robot_parser(url):\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url(url+'/robots.txt')\n",
    "    rp.read()\n",
    "    if not rp.can_fetch(\"*\", url):\n",
    "        a=0\n",
    "    else :\n",
    "        a=1\n",
    "    return a\n",
    "### robot parser\n",
    "\n",
    "\n",
    "\n",
    "# Read URL from command line\n",
    "#url = \"first reference url\"\n",
    "#url1 = \"second reference url\"\n",
    "print (\"===================\")\n",
    "print (\"Page to be crawled:\", url)\n",
    "print( \"===================\")\n",
    "print()\n",
    "\n",
    "# Create queue\n",
    "queue = [(sim_moyen(url),url),]\n",
    "\n",
    "# Maintains list of visited pages\n",
    "visited_list = []\n",
    "# Crawl the page and populate the queue with newly found URLs\n",
    "\n",
    "\n",
    "def crawl(url):\n",
    "    start_time = timeit.default_timer()\n",
    "    visited_list.append((sim_moyen(url),url))\n",
    "    if len(queue) > 15:\n",
    "        return\n",
    "\n",
    "    urlf = urlopen(url)\n",
    "    soup = BeautifulSoup(urlf.read())\n",
    "    urls = soup.findAll(\"a\", href=True)\n",
    "\n",
    "    for i in urls:\n",
    "        flag = 0\n",
    "        # Complete relative URLs and strip trailing slash\n",
    "        complete_url = urljoin(url, i[\"href\"]).rstrip('/')\n",
    "\n",
    "        # Check if the URL already exists in the queue\n",
    "        for j in queue:\n",
    "            if j == complete_url:\n",
    "                flag = 1\n",
    "                break\n",
    "\n",
    "        # If not found in queue\n",
    "        if flag == 0 :\n",
    "            if len(queue) > 15:\n",
    "                \n",
    "                return\n",
    "            if ((visited_list.count(complete_url)) == 0 and sim_moyen(complete_url)<0.95 and robot_parser(complete_url) ==1)  :  # a modifier \n",
    "            \n",
    "                elapsed = timeit.default_timer() + start_time\n",
    "                print(complete_url,elapsed)\n",
    "                queue.append((sim_moyen(complete_url),complete_url))\n",
    "            \n",
    "    # Pop one URL from the queue from the left side so that it can be crawled\n",
    "    queue.sort(reverse=True)\n",
    "    current = queue.pop()[1]\n",
    "    \n",
    "    # Recursive call to crawl until the queue is populated with 100 URLs\n",
    "    crawl(current)\n",
    "\n",
    "crawl(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9304584865855949, 'https://discuss.hackbusters.com/c/phishing/7'), (0.9304584865855949, 'https://discuss.hackbusters.com/c/phishing/7'), (0.9328270275246249, 'https://discuss.hackbusters.com/c/phishing/7?page=1'), (0.9464219114219115, 'https://discuss.hackbusters.com/tag/phishing'), (0.9464219114219115, 'https://discuss.hackbusters.com/tag/phishing'), (0.9464219114219115, 'https://discuss.hackbusters.com/tag/phishing'), (0.9464219114219115, 'https://discuss.hackbusters.com/tag/phishing'), (0.9464219114219115, 'https://discuss.hackbusters.com/tag/phishing'), (0.9464219114219115, 'https://discuss.hackbusters.com/tag/phishing'), (0.9473064243072263, 'https://discuss.hackbusters.com/t/q1-2020-coronavirus-related-phishing-email-attacks-are-up-600/4915'), (0.9473064243072263, 'https://discuss.hackbusters.com/t/q1-2020-coronavirus-related-phishing-email-attacks-are-up-600/4915'), (0.9481250000000001, 'https://discuss.hackbusters.com/t/knowbe4-finds-coronavirus-themed-phishing-spiked-in-q2-2020-infographic/5084'), (0.948766108069244, 'https://discuss.hackbusters.com/t/coronavirus-phish-coming-fast-and-furious-do-you-intend-to-inoculate-your-users-get-the-latest-templates/4856'), (0.948766108069244, 'https://discuss.hackbusters.com/t/coronavirus-phish-coming-fast-and-furious-do-you-intend-to-inoculate-your-users-get-the-latest-templates/4856'), (0.9529834934712984, 'https://discuss.hackbusters.com/t/reporting-a-corporate-phishing-email-with-phisher-activated-it/5152'), (1, 'https://discuss.hackbusters.com/t/heads-up-cybercriminals-launch-phishing-campaign-to-capitalize-on-presidents-health/5174')]\n"
     ]
    }
   ],
   "source": [
    "queue.sort\n",
    "print(queue)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
