{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def getSentences(text):\n",
    "    nlp = English()\n",
    "    nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "    document = nlp(text)\n",
    "    return [sent.string.strip() for sent in document.sents]\n",
    "\n",
    "def printToken(token):\n",
    "    print(token.text, \"->\", token.dep_)\n",
    "\n",
    "def appendChunk(original, chunk):\n",
    "    return original + ' ' + chunk\n",
    "\n",
    "def isRelationCandidate(token):\n",
    "    deps = [\"ROOT\", \"adj\", \"attr\", \"agent\", \"amod\"]\n",
    "    return any(subs in token.dep_ for subs in deps)\n",
    "\n",
    "def isConstructionCandidate(token):\n",
    "    deps = [\"compound\", \"prep\", \"conj\", \"mod\"]\n",
    "    return any(subs in token.dep_ for subs in deps)\n",
    "\n",
    "def processSubjectObjectPairs(tokens):\n",
    "    subject = ''\n",
    "    object = ''\n",
    "    relation = ''\n",
    "    subjectConstruction = ''\n",
    "    objectConstruction = ''\n",
    "    for token in tokens:\n",
    "        printToken(token)\n",
    "        if \"punct\" in token.dep_:\n",
    "            continue\n",
    "        if isRelationCandidate(token):\n",
    "            relation = appendChunk(relation, token.lemma_)\n",
    "        if isConstructionCandidate(token):\n",
    "            if subjectConstruction:\n",
    "                subjectConstruction = appendChunk(subjectConstruction, token.text)\n",
    "            if objectConstruction:\n",
    "                objectConstruction = appendChunk(objectConstruction, token.text)\n",
    "        if \"subj\" in token.dep_:\n",
    "            subject = appendChunk(subject, token.text)\n",
    "            subject = appendChunk(subjectConstruction, subject)\n",
    "            subjectConstruction = ''\n",
    "        if \"obj\" in token.dep_:\n",
    "            object = appendChunk(object, token.text)\n",
    "            object = appendChunk(objectConstruction, object)\n",
    "            objectConstruction = ''\n",
    "\n",
    "    print (subject.strip(), \",\", relation.strip(), \",\", object.strip())\n",
    "    return (subject.strip(), relation.strip(), object.strip())\n",
    "\n",
    "def processSentence(sentence):\n",
    "    tokens = nlp_model(sentence)\n",
    "    return processSubjectObjectPairs(tokens)\n",
    "\n",
    "def printGraph(triples):\n",
    "    G = nx.Graph()\n",
    "    for triple in triples:\n",
    "        G.add_node(triple[0])\n",
    "        G.add_node(triple[1])\n",
    "        G.add_node(triple[2])\n",
    "        G.add_edge(triple[0], triple[1])\n",
    "        G.add_edge(triple[1], triple[2])\n",
    "\n",
    "    pos = nx.spring_layout(G,k=0.15,iterations=20)\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    nx.draw(G, pos, edge_color='black', width=1, linewidths=1,\n",
    "            node_size=500, node_color='seagreen', alpha=0.9,\n",
    "            labels={node: node for node in G.nodes()})\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    text = \"legendary's only mandate was to include monarch, rodan, mothra, and king ghidorah.\"\\\n",
    "            \"ten writers contributed to building on the treatment.\"\\\n",
    "            \"the script took a year to come together.\"\\\n",
    "            \"dougherty also changed, revised, and improved lines during filming and post-production.\"\\\n",
    "            \"due to this, the film became an ensemble piece.\"\\\n",
    "            \"it can't just look like big dinosaurs.\"\\\n",
    "            \"other actors perform the body.\"\\\n",
    "            \"production designer scott chambliss managed all the art directors.the single was released on may 13, 2019.\"\\\n",
    "            \"all tracks are written by bear mccreary, except where noted.\"\\\n",
    "            \"the score is also conducted by mccreary.\"\\\n",
    "            \"on december 10, 2018, the film's first teaser poster and ccxp trailer were released.\"\\\n",
    "            \"in april 2019, the main theatrical poster was released online.\"\\\n",
    "            \"the film was originally scheduled to be released on june 8, 2018.\"\\\n",
    "            \"the collectible tickets were offered in two sizes: standard  and godzilla-sized .\"\\\n",
    "            \"the 4k release includes hdr10, hdr10\"\\\n",
    "            \"the retail exclusives will also include limited special clear files.\"\\\n",
    "            \"such heroes are ready with one-liners, puns, and dry quips.\"\\\n",
    "            \"it was action with a science fiction twist.\"\\\n",
    "            \"currently, action films requiring extensive stunt work and special effects tend to be expensive.\"\\\n",
    "            \"examples include the indiana jones franchise and many superhero films.\"\\\n",
    "            \"themes or elements often prevalent in typical action-horror films\"\\\n",
    "            \"paul blart: mall cop is a recent spoof of this trend .\" \n",
    "\n",
    "    sentences = getSentences(text)\n",
    "    nlp_model = spacy.load('en_core_web_sm')\n",
    "\n",
    "    triples = []\n",
    "    print (text)\n",
    "    for sentence in sentences:\n",
    "        triples.append(processSentence(sentence))\n",
    "\n",
    "    printGraph(triples)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import requests\n",
    "import re,math\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import codecs\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "#This function crawls the hyperlinks fron the list and writes the data into a text file\n",
    "def func(i,name):                                \n",
    "        html = requests.get(i).content\n",
    "        #1 Recoding\n",
    "        unicode_str = html.decode(\"ISO-8859-1\")\n",
    "        encoded_str = unicode_str.encode(\"ascii\",'ignore')\n",
    "        news_soup = BeautifulSoup(encoded_str, \"html.parser\")\n",
    "        a_text = news_soup.find_all('p')\n",
    "        #2 Removing\n",
    "        y=[re.sub(r'<.+?>',r'',str(a)) for a in a_text]\n",
    "\n",
    "        file1 = open('test.txt', 'w')\n",
    "        for item in y:\n",
    "            file1.write(\"%s\\n\" % item)\n",
    "        os.rename(\"test.txt\",name)\n",
    "\n",
    "#This function calculates the csoine similarity between the querry and the documents\n",
    "def get_cosine(vec1, vec2):\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     #print(Counter(words)\n",
    "     return Counter(words)\n",
    "        \n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "\n",
    "#TASK -1\n",
    "#The URL od the wikipedia page of IIT Delhi is taken\n",
    "url = \"https://en.wikipedia.org/wiki/Indian_Institute_of_Technology_Delhi\"\n",
    "response = requests.get(url)\n",
    "data = response.text\n",
    "soup = BeautifulSoup(data, 'lxml')\n",
    "links = []\n",
    "\n",
    "# A list named 'links' is created and the crawler crawls the website and stoores all the hyperlnks in this list \n",
    "for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "    links.append(link.get('href'))\n",
    "print('\\n')\n",
    "print('The hyperlinks found on the wikiperia page of IIT Delhi: ')\n",
    "print(links)\n",
    "print('\\n')\n",
    "\n",
    "#TASK -2\n",
    "#All the hyperlinks are opened one by one and the content is stored in the text file.\n",
    "name=[]\n",
    "for i in range(50):\n",
    "    s = \"file\"+ str(i) +\".txt\" \n",
    "    name.append(s)\n",
    "x=0\n",
    "for i in links:\n",
    "    func(i,name[x])\n",
    "    x=x+1\n",
    "\n",
    "# The stop words are removed and tokenization of the content of tct file takes place\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sentence = []\n",
    "\n",
    "main =[]\n",
    "i=0\n",
    "for item in name:\n",
    "    file1 = codecs.open(item, encoding='utf-8')\n",
    "    word_tokens = word_tokenize(file1.read())\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            s = s + \" \"+w\n",
    "    main.append(s)\n",
    "\n",
    "#TASK-3 \n",
    "# vectorisation for documents and terms take place\n",
    "vectorizer = CountVectorizer()\n",
    "p = vectorizer.fit_transform(main)\n",
    "print('The matrix after vectorization of the documents :')\n",
    "print('\\n')\n",
    "print(p.toarray())\n",
    "print('\\n')\n",
    "\n",
    "#Task-4\n",
    "#The querry is taken fron the user and cosine similarity is calculated between wuerry and every document\n",
    "print('Enter a query: ')\n",
    "\n",
    "all_cos=[]\n",
    "rank=[]\n",
    "text1 = raw_input()\n",
    "vector1 = text_to_vector(text1)\n",
    "for i in range(50):\n",
    "    text2 = codecs.open(name[i], encoding='ISO-8859-1').read()\n",
    "    vector2 = text_to_vector(text2)\n",
    "    cosine = get_cosine(vector1, vector2)\n",
    "    all_cos.append(cosine)\n",
    "    rank.append(cosine)\n",
    "    print ('Cosine:', i, cosine)\n",
    "\n",
    "rank.sort(reverse=True)\n",
    "\n",
    "#TASK-5\n",
    "# The rank od document based on similarity and the url of top 10 documents are displayed \n",
    "print('Rank of documents based on similarity is as follows:')\n",
    "print('\\n')\n",
    "for i in range(50):\n",
    "    print ('Rank:', i,': ', rank[i])\n",
    "\n",
    "j = 1\n",
    "while j < 11:\n",
    "    maxpos= all_cos.index(max(all_cos)) \n",
    "    s = all_cos[maxpos]\n",
    "    print('\\n')\n",
    "    print ('Document ',j)\n",
    "    print('Value of similarity :',s)\n",
    "    print ('URL for that page is :',links[maxpos])\n",
    "    print('\\n')\n",
    "    all_cos.remove(s)\n",
    "    j += 1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import argparse\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from collections import Counter, defaultdict\n",
    "from math import log10\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "\n",
    "prob = 0.05\n",
    "target_delta = 0.04\n",
    "\n",
    "stop_words = [\n",
    "    'a', 'also', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'do',\n",
    "    'for', 'have', 'is', 'in', 'it', 'of', 'or', 'see', 'so',\n",
    "    'that', 'the', 'this', 'to', 'we'\n",
    "]\n",
    "\n",
    "#Taking the input\n",
    "#Search query and list of urls\n",
    "def input():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=__doc__,\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        'indian',\n",
    "        type=str,\n",
    "        nargs='+',\n",
    "        help='Search query string can contain multiple words'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        'https://en.wikipedia.org/wiki/Indian_Institute_of_Technology_Delhi',\n",
    "        type=str,\n",
    "        nargs='+',\n",
    "        help='At least one seed url for the crawler to start from'\n",
    "    )\n",
    "    return parser.parse_args()\n",
    "\n",
    "def crawler(urls, _frontier={}, _bases=None):\n",
    "    '''\n",
    "    Takes a list of urls as argument and crawls them recursively until\n",
    "    no new url can be found.\n",
    "    Returns a sorted list of tuples (url, content, links).\n",
    "    `links` is a list of urls.\n",
    "    '''\n",
    "    if not _bases:\n",
    "        _bases = [urlparse(u).netloc for u in urls]\n",
    "    for url in [u.rstrip('/') for u in urls]:\n",
    "        if url in _frontier:\n",
    "            continue\n",
    "        try:\n",
    "            response = urlopen(url)\n",
    "        except HTTPError as e:\n",
    "            print(e, url)\n",
    "            continue\n",
    "\n",
    "        page = parse(response, url, _bases)\n",
    "        print('crawled %s with %s links' % (url, len(page[2])))\n",
    "        _frontier[url] = page\n",
    "        crawler(page[2], _frontier, _bases)\n",
    "    return sorted(_frontier.values())\n",
    "\n",
    "\n",
    "def parse(html, url, bases):\n",
    "    '''\n",
    "    Takes an html string and a url as arguments.\n",
    "    Returns a tuple (url, content, links) parsed from the html.\n",
    "    '''\n",
    "    try:\n",
    "        data = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        content = data.body.get_text().strip()\n",
    "\n",
    "        links = [urljoin(url, l.get('href')) for l in data.findAll('a')]\n",
    "        links = [l for l in links if urlparse(l).netloc in bases]\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    return url, content, links\n",
    "\n",
    "def page_rank(pages):\n",
    "    '''\n",
    "    Returns a matrix with documents as columns\n",
    "    and values for each round as rows.\n",
    "    Number of rows depends on how long it takes to reach the target_delta.\n",
    "    '''\n",
    "    N = len(pages)\n",
    "    transition_matrix = create_transition_matrix(pages)\n",
    "    ranks_in_steps = [[1 / N] * N]\n",
    "    while True:\n",
    "        possibilities = ranks_in_steps[-1] * transition_matrix\n",
    "        delta = get_delta(possibilities, ranks_in_steps[-1])\n",
    "        ranks_in_steps.append(np.squeeze(np.asarray(possibilities)))\n",
    "        if delta <= target_delta:\n",
    "            return ranks_in_steps\n",
    "\n",
    "def create_transition_matrix(pages):\n",
    "    '''\n",
    "    Returns a matrix with document urls as rows\n",
    "    and document links as columns.\n",
    "    Each cell contains the probability for a document\n",
    "    to transition to a link.\n",
    "    '''\n",
    "    links = get_links(pages)\n",
    "    urls = get_urls(pages)\n",
    "    N = len(pages)\n",
    "    m = np.matrix([[weight_link(N, u, l) for u in urls] for l in links])\n",
    "    return prob_to_transition(N, m)\n",
    "\n",
    "\n",
    "def weight_link(N, url, links):\n",
    "    if not links:\n",
    "        return 1 / N\n",
    "    if url in links:\n",
    "        return 1 / len(links)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def prob_to_transition(N, m):\n",
    "    return m * (1 - prob) + prob / N\n",
    "\n",
    "\n",
    "def get_delta(a, b):\n",
    "    return np.abs(a - b).sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_links(pages):\n",
    "    return [links for url, content, links in pages]\n",
    "\n",
    "\n",
    "def best_rank(ranks, pages):\n",
    "    '''\n",
    "    Returns a dict with document urls as keys\n",
    "    and their ranks as values.\n",
    "    '''\n",
    "    list_of_url = [url for url, content , links in pages]\n",
    "    return dict(zip(list_of_url, ranks[-1]))\n",
    "\n",
    "# Index\n",
    "\n",
    "def create_index(pages):\n",
    "    '''\n",
    "    Returns the index as a dict with terms as keys\n",
    "    and lists tuples(url, count) as values.\n",
    "    Count says how many times the term occured in the document.\n",
    "    '''\n",
    "    index = defaultdict(list)\n",
    "    for url, content, links in pages:\n",
    "        counts = count_terms(content)\n",
    "        for term, count in counts.items():\n",
    "            index[term].append((url, count))\n",
    "    return index\n",
    "\n",
    "\n",
    "def count_terms(content):\n",
    "    '''\n",
    "    content is a text string.\n",
    "    Returns a Counter with terms as keys\n",
    "    and their occurence as values.\n",
    "    '''\n",
    "    return Counter(get_terms(content))\n",
    "\n",
    "\n",
    "normalize = re.compile('[^a-z0-9]+')\n",
    "\n",
    "\n",
    "def get_terms(s):\n",
    "    '''\n",
    "    Get a list of terms from a string.\n",
    "    Terms are lower case and all special characters are removed.\n",
    "    '''\n",
    "    normalized = [normalize.sub('', t.lower()) for t in s.split()]\n",
    "    return [t for t in normalized if t not in stop_words]\n",
    "\n",
    "\n",
    "def weight_index(index, N):\n",
    "    '''\n",
    "    Takes an index as first argument\n",
    "    and the total number of documents as second argument.\n",
    "    Returns a new index with tf_idf weights instead of simple counts.\n",
    "    '''\n",
    "    weighted_index = defaultdict(list)\n",
    "    for term, docs in index.items():\n",
    "        df = len(docs)\n",
    "        for url, count in docs:\n",
    "            weight = tf_idf(count, N, df)\n",
    "            weighted_index[term].append((url, weight))\n",
    "    return weighted_index\n",
    "\n",
    "\n",
    "def tf_idf(tf, N, df):\n",
    "    return 1 + log10(tf) * log10(N / df)\n",
    "\n",
    "#word term frequency -  1 + log10(tf)\n",
    "#in document frequency -  log10(N / df)\n",
    "\n",
    "\n",
    "def normalize_index(index):\n",
    "    '''\n",
    "    Takes an index as argument.\n",
    "    Returns a new index with normalized weights.\n",
    "    '''\n",
    "    lengths = doc_lengths(index)\n",
    "    norm_index = defaultdict(list)\n",
    "    for term, docs in index.items():\n",
    "        for url, weight in docs:\n",
    "            norm_index[term].append((url, weight / lengths[url]))\n",
    "    return norm_index\n",
    "\n",
    "\n",
    "def doc_lengths(index):\n",
    "    '''\n",
    "    Returns a dict with document urls as keys\n",
    "    and vector lengths as values.\n",
    "    The length is calculated using the vector of weights\n",
    "    for the terms in the document.\n",
    "    '''\n",
    "    doc_vectors = defaultdict(list)\n",
    "    for docs in index.values():\n",
    "        for url, weight in docs:\n",
    "            doc_vectors[url].append(weight)\n",
    "    return {url: np.linalg.norm(doc) for url, doc in doc_vectors.items()}\n",
    "\n",
    "\n",
    "# Search & Scoring\n",
    "\n",
    "def cosine_similarity(index, N, query):\n",
    "    '''\n",
    "    query is a string of terms.\n",
    "    Returns a sorted list of tuples (url, score).\n",
    "    Score is calculated using the cosine distance\n",
    "    between document and query.\n",
    "    '''\n",
    "    scores = defaultdict(int)\n",
    "    print(query)\n",
    "    terms = query[0]\n",
    "    qw = {t: tf_idf(1, N, len(index[t])) for t in terms if t in index}\n",
    "    query_len = np.linalg.norm(list(qw.values()))\n",
    "    for term in qw:\n",
    "        query_weight = qw[term] / query_len\n",
    "        for url, weight in index[term]:\n",
    "            scores[url] += weight * query_weight\n",
    "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "def combined_search(index, N, rank, query):\n",
    "    '''\n",
    "    Returns a sorted list of tuples (url, score).\n",
    "    Score is the product of the cosine similarity and the PageRank.\n",
    "    '''\n",
    "    scores = cosine_similarity(index, N, query)\n",
    "    combined = [(doc, score * rank[doc]) for doc, score in scores]\n",
    "    return sorted(combined, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "def print_combined_search(index, N, rank, query):\n",
    "    print('Search results for \"%s\":' % (query))\n",
    "    for url, score in combined_search(index, N, rank, query):\n",
    "        print('%.6f  %s' % (score, url))\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = input()\n",
    "    # Computing\n",
    "    pages = crawler(args.url)\n",
    "    ranks = page_rank(pages)\n",
    "    rank = best_rank(ranks, pages)\n",
    "    N = len(pages)\n",
    "    index = create_index(pages)\n",
    "    weighted_index = weight_index(index, N)\n",
    "    norm_index = normalize_index(weighted_index)\n",
    "\n",
    "    # Print results\n",
    "    print()\n",
    "    print('Number of pages:', len(pages))\n",
    "    print('Terms in index:', len(index))\n",
    "    print('Iterations for PageRank:', len(ranks))\n",
    "    print()\n",
    "    print_combined_search(norm_index, N, rank, args.query)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import requests\n",
    "import re,math\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import codecs\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "#This function crawls the hyperlinks fron the list and writes the data into a text file\n",
    "def func(i,name):                                \n",
    "        html = requests.get(i).content\n",
    "        #1 Recoding\n",
    "        unicode_str = html.decode(\"ISO-8859-1\")\n",
    "        encoded_str = unicode_str.encode(\"ascii\",'ignore')\n",
    "        news_soup = BeautifulSoup(encoded_str, \"html.parser\")\n",
    "        a_text = news_soup.find_all('p')\n",
    "        #2 Removing\n",
    "        y=[re.sub(r'<.+?>',r'',str(a)) for a in a_text]\n",
    "\n",
    "        file1 = open('test.txt', 'w')\n",
    "        for item in y:\n",
    "            file1.write(\"%s\\n\" % item)\n",
    "        os.rename(\"test.txt\",name)\n",
    "\n",
    "#This function calculates the csoine similarity between the querry and the documents\n",
    "def get_cosine(vec1, vec2):\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     #print(Counter(words)\n",
    "     return Counter(words)\n",
    "        \n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "\n",
    "#TASK -1\n",
    "#The URL od the wikipedia page of IIT Delhi is taken\n",
    "url = \"https://en.wikipedia.org/wiki/Indian_Institute_of_Technology_Delhi\"\n",
    "response = requests.get(url)\n",
    "data = response.text\n",
    "soup = BeautifulSoup(data, 'lxml')\n",
    "links = []\n",
    "\n",
    "# A list named 'links' is created and the crawler crawls the website and stoores all the hyperlnks in this list \n",
    "for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "    links.append(link.get('href'))\n",
    "print('\\n')\n",
    "print('The hyperlinks found on the wikiperia page of IIT Delhi: ')\n",
    "print(links)\n",
    "print('\\n')\n",
    "\n",
    "#TASK -2\n",
    "#All the hyperlinks are opened one by one and the content is stored in the text file.\n",
    "name=[]\n",
    "for i in range(50):\n",
    "    s = \"file\"+ str(i) +\".txt\" \n",
    "    name.append(s)\n",
    "x=0\n",
    "for i in links:\n",
    "    func(i,name[x])\n",
    "    x=x+1\n",
    "\n",
    "# The stop words are removed and tokenization of the content of tct file takes place\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sentence = []\n",
    "\n",
    "main =[]\n",
    "i=0\n",
    "for item in name:\n",
    "    file1 = codecs.open(item, encoding='utf-8')\n",
    "    word_tokens = word_tokenize(file1.read())\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            s = s + \" \"+w\n",
    "    main.append(s)\n",
    "\n",
    "#TASK-3 \n",
    "# vectorisation for documents and terms take place\n",
    "vectorizer = CountVectorizer()\n",
    "p = vectorizer.fit_transform(main)\n",
    "print('The matrix after vectorization of the documents :')\n",
    "print('\\n')\n",
    "print(p.toarray())\n",
    "print('\\n')\n",
    "\n",
    "#Task-4\n",
    "#The querry is taken fron the user and cosine similarity is calculated between wuerry and every document\n",
    "print('Enter a query: ')\n",
    "\n",
    "all_cos=[]\n",
    "rank=[]\n",
    "text1 = raw_input()\n",
    "vector1 = text_to_vector(text1)\n",
    "for i in range(50):\n",
    "    text2 = codecs.open(name[i], encoding='ISO-8859-1').read()\n",
    "    vector2 = text_to_vector(text2)\n",
    "    cosine = get_cosine(vector1, vector2)\n",
    "    all_cos.append(cosine)\n",
    "    rank.append(cosine)\n",
    "    print 'Cosine:', i, cosine\n",
    "\n",
    "rank.sort(reverse=True)\n",
    "\n",
    "#TASK-5\n",
    "# The rank od document based on similarity and the url of top 10 documents are displayed \n",
    "print('Rank of documents based on similarity is as follows:')\n",
    "print('\\n')\n",
    "for i in range(50):\n",
    "    print 'Rank:', i,': ', rank[i]\n",
    "\n",
    "j = 1\n",
    "while j < 11:\n",
    "    maxpos= all_cos.index(max(all_cos)) \n",
    "    s = all_cos[maxpos]\n",
    "    print('\\n')\n",
    "    print 'Document ',j\n",
    "    print'Value of similarity :',s\n",
    "    print 'URL for that page is :',links[maxpos]\n",
    "    print('\\n')\n",
    "    all_cos.remove(s)\n",
    "    j += 1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import pprint\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from search_engine_parser.core.engines.yahoo import Search as YahooSearch\n",
    "search_args = ('preaching to the choir', 1)\n",
    "ysearch = YahooSearch()\n",
    "yresults = ysearch.search(*search_args)\n",
    "  # print 10th link from yahoo search\n",
    "print(yresults[\"links\"][4])\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try: \n",
    "    from googlesearch import search \n",
    "except ImportError: \n",
    "    print(\"No module named 'google' found\") \n",
    "\n",
    "# to search \n",
    "query = \"phishing\"\n",
    "tab=[]\n",
    "for j in search(query, tld=\"com\", num=2, stop=2, pause=10): \n",
    "    tab.append(j)\n",
    "print(tab)\n",
    "with open('tes.txt', 'w') as f:\n",
    "    for item in tab:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_links_recursive(base, path, visited, max_depth=3, depth=0):\n",
    "    if depth < max_depth:\n",
    "        try:\n",
    "            soup = BeautifulSoup(requests.get(base + path).text, \"html.parser\")\n",
    "\n",
    "            for link in soup.find_all(\"a\"):\n",
    "                href = link.get(\"href\")\n",
    "\n",
    "                if href not in visited:\n",
    "                    visited.add(href)\n",
    "                    print(f\"at depth {depth}: {href}\")\n",
    "\n",
    "                    if href.startswith(\"http\"):\n",
    "                        get_links_recursive(href, \"\", visited, max_depth, depth + 1)\n",
    "                    else:\n",
    "                        get_links_recursive(base, href, visited, max_depth, depth + 1)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "get_links_recursive(\"http://toscrape.com\", \"\", set([\"http://toscrape.com\"]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "\n",
    "visited = set([\"http://toscrape.com\"])\n",
    "dq = deque([[\"http://toscrape.com\", \"\", 0]])\n",
    "max_depth = 3\n",
    "\n",
    "while dq:\n",
    "    base, path, depth = dq.popleft()\n",
    "    #                         ^^^^ removing \"left\" makes this a DFS (stack)\n",
    "\n",
    "    if depth < max_depth:\n",
    "        try:\n",
    "            soup = BeautifulSoup(requests.get(base + path).text, \"html.parser\")\n",
    "\n",
    "            for link in soup.find_all(\"a\"):\n",
    "                href = link.get(\"href\")\n",
    "                \n",
    "                if href not in visited:\n",
    "                    visited.add(href)\n",
    "                    print(\"  \" * depth + f\"at depth {depth}: {href}\")\n",
    "\n",
    "                    if href.startswith(\"http\"):\n",
    "                        dq.append([href, \"\", depth + 1])\n",
    "                    else:\n",
    "                        dq.append([base, href, depth + 1])\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Page to be crawled: https://www.sharecare.com/health/wellness-healthy-living/what-is-health-and-wellness\n",
      "===================\n",
      "\n",
      "https://www.sharecare.com\n",
      "https://www.sharecare.com/health/allergy\n",
      "https://www.sharecare.com/health/cancer\n",
      "https://www.sharecare.com/covid19\n",
      "https://www.sharecare.com/health/type-2-diabetes\n",
      "https://www.sharecare.com/health/heart-disease\n",
      "[('https://www.sharecare.com', 0), ('https://www.sharecare.com/health/allergy', 1), ('https://www.sharecare.com/health/cancer', 2), ('https://www.sharecare.com/covid19', 3), ('https://www.sharecare.com/health/type-2-diabetes', 4), ('https://www.sharecare.com/health/heart-disease', 5)]\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from urllib.request import urlopen\n",
    "import sys\n",
    "import urllib\n",
    "import urllib.robotparser\n",
    "# Read URL from command line\n",
    "url = \"https://www.sharecare.com/health/wellness-healthy-living/what-is-health-and-wellness\"\n",
    "\n",
    "print (\"===================\")\n",
    "print (\"Page to be crawled:\", url)\n",
    "print( \"===================\")\n",
    "print()\n",
    "\n",
    "def robot_parser(url):\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url(url+'/robots.txt')\n",
    "    rp.read()\n",
    "    if not rp.can_fetch(\"*\", url):\n",
    "        a=0\n",
    "    else :\n",
    "        a=1\n",
    "    return a\n",
    "\n",
    "\n",
    "\n",
    "# Create queue\n",
    "queue = []\n",
    "\n",
    "# Maintains list of visited pages\n",
    "visited_list = []\n",
    "\n",
    "\n",
    "# Crawl the page and populate the queue with newly found URLs\n",
    "def crawl(url):\n",
    "    visited_list.append((url,len(queue)))\n",
    "    if len(queue) > 5:\n",
    "        return\n",
    "    z=0\n",
    "    urlf = urlopen(url)\n",
    "    soup = BeautifulSoup(urlf.read())\n",
    "    urls = soup.findAll(\"a\", href=True)\n",
    "   \n",
    "    for i in urls:\n",
    "      \n",
    "        \n",
    "        flag = 0\n",
    "        \n",
    "        # Complete relative URLs and strip trailing slash\n",
    "        complete_url = urljoin(url, i[\"href\"]).rstrip('/')\n",
    "        # Check if the URL already exists in the queue\n",
    "        \n",
    "        for j in queue:  \n",
    "            if j == complete_url:             \n",
    "                flag = 1\n",
    "                break\n",
    "\n",
    "        # If not found in queue\n",
    "        if flag == 0:\n",
    "            if len(queue) > 5: \n",
    "                return\n",
    "            if (visited_list.count(complete_url)) == 0 and robot_parser(complete_url)==1:\n",
    "                \n",
    "                print(complete_url)  # a modifier \n",
    "                queue.append((complete_url,len(queue)))\n",
    "            \n",
    "    # Pop one URL from the queue from the left side so that it can be crawled\n",
    "    queue.sort()\n",
    "    current = queue.pop(0)\n",
    "    # Recursive call to crawl until the queue is populated with 100 URLs\n",
    "    crawl(current)\n",
    "\n",
    "crawl(url)\n",
    "print(queue)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4139\n"
     ]
    }
   ],
   "source": [
    "tel = {'jack':4098, 'sape':4139}\n",
    "print(tel['sape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "riya\n"
     ]
    }
   ],
   "source": [
    "customers =[]\n",
    "customers.append((2,\"Harry\"))\n",
    "customers.append((3,\"charles\"))\n",
    "customers.append((1,\"riya\"))\n",
    "customers.sort(reverse=True)\n",
    "print( customers.pop()[1])\n",
    "#i = 5 + tup()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bon\n"
     ]
    }
   ],
   "source": [
    "import urllib.robotparser\n",
    "def robot_parser(url):\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url(url+'/robots.txt')\n",
    "    rp.read()\n",
    "    if not rp.can_fetch(\"*\", url):\n",
    "        print('error')\n",
    "    else :\n",
    "        print('bon')\n",
    "robot_parser('https://www.sharecare.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
